[["index.html", "Generative Modeling Workshop 1 Index", " Generative Modeling Workshop Susan Holmes 1 Index "],["workshop-on-using-generative-models.html", "2 Workshop on using generative models 2.1 Our learning goals: 2.2 Prerequisites:", " 2 Workshop on using generative models Lisbon, 7, 8 May, 2025 2.1 Our learning goals: Learn how to use simulation tools to solve and understand statistical models. Go over examples of experimental design using generative models. Use different R packages to simulate data according to a hierarchical model with multiple levels of random variation. 2.2 Prerequisites: Some statistical know-how and R and RStudio installed on your laptops. Some useful buzzwords and acronyms to look up in case: GOF: Goodness of Fit MCMC: Markov chain Monte Carlo ABC: Approximate Bayesian Computation Random generation: From uniform and non-uniform distributions. Mixtures: Finite combinations of known distributions Hierarchical Models: Infinite mixtures GANs: Generative Adversarial network Identifiability GMM : Generalized Mixed Models GLM : Generalized Linear Models HMC: Hamiltonian Monte Carlo "],["beyond-black-box-simulation.html", "3 Beyond Black Box Simulation 3.1 New Lingua Franca of Science", " 3 Beyond Black Box Simulation Beyond Black Box Simulation Lisbon Nova [7 | May | 2025] Paper: https://go.wisc.edu/833zs8 Code: https://go.wisc.edu/7222i9 Susan Holmes Stanford University joint work with, Kris Sankaran, UW-Madison 3.1 New Lingua Franca of Science Simulators have emerged as a general problem-solving device across various domains, many of which now have rich, open-source libraries. Where is the interface with statistics? Experimental design, model building, and decision-making. The E3SM is used for long-term climate projections. 3.1.1 New Lingua Franca of Science Simulators have emerged as a general problem-solving device across various domains, many of which now have rich, open-source libraries. Where is the interface with statistics? Experimental design, model building, and decision-making. Splatter generates synthetic single-cell genomics data. 3.1.2 Grammar of Generative Models Transparent simulators can be built by interactively composing simple modules. Probabilistic programming has simplified the process. Regression b: Hierarchy c: Latent Structure d: Temporal Variation 3.1.3 Discrepancy and Iterability By learning a discriminator to contrast real vs. simulated data, we can systematically improve the assumed generative mechanism. 3.1.4 Experimental Design Renaissance Let’s consider a microbiomics case study: To block or not to block? Blocking removes person-level effects… …but increases participant burden. 3.1.5 Simulation to the Rescue How can we navigate trade-offs like this? Simulate! Simulators provide data for more precise decision-making. 3.1.6 Covasim Following the outbreak of COVID-19, the research community came together to build simulators that could inform pandemic response. * E.g., “What would happen if we held classes remotely for two weeks?” 3.1.7 Covasim Covasim is an example of an agent-based model. Starting from local interaction rules, it lets us draw global inferences. Statistical emulators mimic the relationship between input hyperparameters and output data, substantially reducing the computational burden. Inference and imagination: Statistical calibration grounds us in reality while generative tinkering encourages us to imagine. 3.1.8 What are we going to study ? Many examples, like showing non-identifiability, the evolution of mimicry in butterflies, longitudinal study design, the duality between agents and particles, … * Paper Link: https://go.wisc.edu/833zs8 * Code (R + Python + NetLogo): https://go.wisc.edu/7222i9 A github repository of the material is available here: https://spholmes.github.io/Generative_Modeling/ "],["slides.html", "4 Slides", " 4 Slides NotBlackBoxSimulation Hierarchical Models Experimental Design Identifiability "],["labs-and-challenges.html", "5 Labs and Challenges :", " 5 Labs and Challenges : Find code for an example of a hierarchical generative model in the book: Modern Statistics for Modern Biology Install the package generative from the github repository spholme/generativeusing the packagedevtools` with devtools::install_github(&quot;spholmes/generative&quot;) Try out examples from the repository of the paper at: https://go.wisc.edu/833zs8 Code: https://go.wisc.edu/7222i9 Install Stan : you can ask a LLM to help for instance chatGPT provided: https://chatgpt.com/share/681bc493-1a64-8005-8d07-cc77cff145d7 "],["intervention.html", "6 Intervention 6.1 Start defining the intervention in a time series", " 6 Intervention 6.1 Start defining the intervention in a time series library(tidyverse) library(splines) library(generative) set.seed(123) times &lt;- seq(-10, 10, length.out = 200) pf &lt;- peak_fun(times, 1, 10, 5) plot(times, pf(times), type = &quot;l&quot;) u &lt;- runif(20, -10, 10) points(u, gaussian_noise(pf, u)) u &lt;- seq(-10, 10, length.out = 10) intervals &lt;- cbind(head(u, -1), tail(u, -1)) weights &lt;- 5:1 weights &lt;- weights / sum(weights) locs &lt;- weighted_sampling(1e4, intervals, weights) hist(locs) We now have a way of simulating parameterized mean functions and observations with noise around them We also have ways of sampling locations to observe the function, based on a weighted combination of uniforms We need a way of estimating these mean functions and evaluating the resulting fit. We also need to evaluate many combinations of weightings, either using random search or bayesian optimization First, we attempt to estimate these mean functions using simple piecewise linear splines w &lt;- c(1, 1, 1, 1, 3, 4, 3, 1, 1) w &lt;- w / sum(w) u &lt;- weighted_sampling(40, intervals, w) y &lt;- gaussian_noise(pf, u) fit &lt;- lm(y ~ bs(u, df = 6, degree = 1, Boundary.knots = c(-10, 10))) plot(times, pf(times), type = &quot;l&quot;) points(u, y, col = &quot;red&quot;) points(u, predict(fit), col = &quot;blue&quot;) f_hat &lt;- predict(fit, newdata = data.frame(u = times)) points(times, f_hat, col = &quot;blue&quot;) B &lt;- 1e3 mse &lt;- matrix(nrow = B, ncol = 2) for (b in seq_len(B)) { # random u&#39;s u &lt;- runif(50, -10, 10) mse[b, 1] &lt;- spline_mse(u, times, pf) u &lt;- weighted_sampling(50, intervals, w) mse[b, 2] &lt;- spline_mse(u, times, pf) } colnames(mse) &lt;- c(&quot;uniform&quot;, &quot;weighted&quot;) mse &lt;- as_tibble(mse) %&gt;% mutate(id = 1:n()) %&gt;% pivot_longer(-id, names_to = &quot;sampling&quot;, values_to = &quot;mse&quot;) ggplot(mse) + geom_histogram( aes(mse, fill = sampling), alpha = 0.8, position = &quot;identity&quot;, bins = 50 ) + scale_y_continuous(expand = c(0, 0, 0.1, 0)) + scale_x_continuous(expand = c(0, 0)) "],["intervention_optimize.html", "7 Intervention_Optimize", " 7 Intervention_Optimize (this can be slow) library(tidyverse) library(laGP) library(purrr) library(generative) library(splines) review_theme() set.seed(123) ### This is very slow n_init &lt;- 100 n_interval &lt;- 7 intervals &lt;- make_intervals(n_interval) times &lt;- seq(-10, 10, length.out = 200) pf &lt;- peak_fun(times, 1, 10, 5) # initial sample of weights betas &lt;- rerun(n_init, rnorm(n_interval, 0, 1)) losses &lt;- map_dbl(betas, ~ evaluate_weights(., intervals, times, pf, N = 25)) We compute the main Bayesian optimization loop below. n_test &lt;- 500 n_add &lt;- 10 n_iters &lt;- 50 for (i in seq_len(n_iters)) { # fit a GP to the betas / losses already computed beta_norm &lt;- scale_list(betas) beta_test &lt;- rerun(n_test, runif(n_interval, -4, 4)) # find most promising new test points y_hat &lt;- aGP(beta_norm, losses, scale_list(beta_test), verb = 0) sort_ix &lt;- order(y_hat$mean - y_hat$var) # evaluate loss on new test points new_betas &lt;- beta_test[sort_ix][1:n_add] betas &lt;- c(betas, new_betas) losses &lt;- c( losses, map_dbl(new_betas, ~ evaluate_weights(., intervals, times, pf, N = 25)) ) } Here are some of the weightings that we should use for longitudinal sampling. They generally favor more samples in the region where the peak is located. w &lt;- bind_rows(map(betas, ~ data.frame(exp(t(.))))) w &lt;- w / rowSums(w) w$loss &lt;- losses library(embed) library(tidymodels) w_reduce &lt;- recipe(~ ., data = w) %&gt;% update_role(loss, new_role = &quot;id&quot;) umap_rec &lt;- step_umap(w_reduce, all_predictors(), neighbors = 30, min_dist = 0.05) scores &lt;- prep(umap_rec) %&gt;% juice() %&gt;% bind_cols(w %&gt;% select(-loss)) ggplot(scores) + geom_point(aes(UMAP1, UMAP2, col = log(loss), size = X4)) + scale_color_viridis_c(direction = -1, limits = c(0, 8), option = &quot;inferno&quot;) w_df &lt;- w %&gt;% mutate(id = row_number()) %&gt;% arrange(-loss) %&gt;% pivot_longer(starts_with(&quot;X&quot;), names_to = &quot;component&quot;) intervals_df &lt;- as.data.frame(intervals) intervals_df$component &lt;- paste0(&quot;X&quot;, 1:nrow(intervals)) w_df &lt;- w_df %&gt;% left_join(intervals_df) ggplot(w_df %&gt;% filter(log(loss) &lt; 0.08)) + geom_rect(aes(xmin = V1, xmax = V2, ymin = 0, ymax = value), fill = &quot;#262526&quot;) + scale_x_continuous(expand = c(0, 0), breaks = c(-5, 0, 5)) + scale_y_continuous(expand = c(0, 0, 0.1, 0)) + facet_grid(. ~ round(loss, 4)) ggsave(&quot;~/Downloads/best_weights.png&quot;) u &lt;- map(1:8, ~ runif(25, -10, 10)) examples &lt;- map_dfr(u, ~ data.frame( u = ., y = gaussian_noise(pf, .) ), .id = &quot;run&quot; ) %&gt;% mutate(run = as.factor(run)) preds &lt;- examples %&gt;% split(.$run) %&gt;% map_dfr(~ { fit &lt;- lm(y ~ bs(u, df = 6, degree = 1, Boundary.knots = c(-10, 10)), data = .) data.frame( times = times, mu = pf(times), f_hat = predict(fit, newdata = data.frame(u = times)) ) }, .id = &quot;run&quot; ) %&gt;% group_by(run) %&gt;% mutate( run = as.factor(run), loss = (f_hat - mu) ^ 2 ) truth &lt;- data.frame(times = times, mu = pf(times)) run_order &lt;- preds %&gt;% group_by(run) %&gt;% summarise(mse = mean(loss)) %&gt;% arrange(mse) %&gt;% pull(run) preds &lt;- preds %&gt;% mutate(run = factor(run, levels = run_order)) examples &lt;- examples %&gt;% mutate(run = factor(run, levels = run_order)) ggplot(preds) + geom_line(aes(times, mu), col = &quot;#F27507&quot;, linewidth = 2) + geom_line(aes(times, f_hat), col = &quot;#087F8C&quot;, linewidth = 1.5) + geom_point(data = examples, aes(u, y), col = &quot;#221F26&quot;) + labs(y = &quot;y&quot;, x = &quot;time&quot;) + facet_wrap(~ run, ncol = 4) ggsave(&quot;~/Downloads/peak_samples.png&quot;, dpi = 800) "],["heteroskedastic-mixtures-goodness-of-fit-iterations.html", "8 Heteroskedastic Mixtures : goodness of fit iterations", " 8 Heteroskedastic Mixtures : goodness of fit iterations library(tidyverse) library(caret) library(scico) library(generative) review_theme() set.seed(090322) Simulate data from a heteroskedastic mixture of t’s Fit a standard GMM (stan) Fit a discriminator (caret) Visualize discriminator probabilities Fit updated heteroskedastic model Repeat discrimination Fit true model Repeat discrimination disc_data &lt;- list() p &lt;- list() x &lt;- t_mixture(500) ggplot(x) + geom_point(aes(V1, V2)) + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) + coord_fixed() ggsave(&quot;~/Downloads/true_mixture.png&quot;, dpi = 400, width = 5) fit &lt;- fit_gmm(x, K = 4) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.000723 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 7.23 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2745.118 1.000 1.000 ## 200 -2427.204 0.565 1.000 ## 300 -2427.541 0.377 0.131 ## 400 -2425.820 0.283 0.131 ## 500 -2425.803 0.226 0.001 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 1.7 seconds. x_sim &lt;- mixture_predictive(fit) ggplot(x_sim) + geom_point(aes(`1`, `2`), size = 0.5, alpha = 0.8) + facet_wrap(~ .iteration) + coord_fixed() disc_data[[&quot;gmm&quot;]] &lt;- bind_dicriminative(x, x_sim) gbmGrid &lt;- expand.grid( interaction.depth = 5, n.trees = 100, shrinkage = 0.1, n.minobsinnode = 2 ) trControl &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 10 ) fit &lt;- train( x = disc_data[[&quot;gmm&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;gmm&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) ## 1 package is needed and is not installed. (gbm). Would you like to try to install it now? ## 1: yes ## 2: no ## ## ## The downloaded binary packages are in ## /var/folders/9m/6136fr3s7619m_62rh3bvb8r0000gr/T//RtmpllcfsR/downloaded_packages fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.543 0.086 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was held constant at ## a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning parameter &#39;n.minobsinnode&#39; was ## held constant at a value of 2 p[[&quot;gmm&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;gmm&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;gmm&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) fit &lt;- fit_gmm_cov(x, K = 5) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.002066 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 20.66 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2451.708 1.000 1.000 ## 200 -2445.865 0.501 1.000 ## 300 -2445.244 0.334 0.002 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 3.5 seconds. x_sim &lt;- mixture_predictive(fit) disc_data[[&quot;gmm_cov&quot;]] &lt;- bind_dicriminative(x, x_sim) fit &lt;- train( x = disc_data[[&quot;gmm_cov&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;gmm_cov&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.5253 0.0506 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was held constant at ## a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning parameter &#39;n.minobsinnode&#39; was ## held constant at a value of 2 p[[&quot;gmm_cov&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;gmm_cov&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;gmm_cov&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) fit &lt;- fit_tmm(x, K = 5) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.002041 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 20.41 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2655.391 1.000 1.000 ## 200 -2649.922 0.501 1.000 ## 300 -2637.090 0.336 0.005 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 3.3 seconds. x_sim &lt;- mixture_predictive(fit) ggplot(x_sim) + geom_point(aes(`1`, `2`), size = 0.5, alpha = 0.8) + facet_wrap(~ .iteration) + coord_fixed() disc_data[[&quot;tmm&quot;]] &lt;- bind_dicriminative(x, x_sim, iter = 4) fit &lt;- train( x = disc_data[[&quot;tmm&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;tmm&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.5844 0.1688 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was held constant at ## a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning parameter &#39;n.minobsinnode&#39; was ## held constant at a value of 2 p[[&quot;tmm&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;tmm&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;tmm&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) p_df &lt;- bind_rows(p, .id = &quot;model&quot;) %&gt;% mutate( model = fct_recode(model, &quot;Gaussian (Shared Covariance)&quot; = &quot;gmm&quot;, &quot;Gaussian (Individual Covariance)&quot; = &quot;gmm_cov&quot;, &quot;Student&#39;s t (Individual Covariance)&quot; = &quot;tmm&quot;), y_label = factor(y_label, levels = c(&quot;True Data&quot;, &quot;Posterior Samples&quot;)) ) ggplot(p_df, aes(V1, V2, col = prob.true)) + geom_point(size = 1.2) + scale_color_scico(palette = &quot;berlin&quot;) + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;, col = &quot;Predicted Probability&quot;) + xlim(-11, 11) + coord_fixed() + facet_grid(y_label ~ model) ggsave(&quot;~/Downloads/modeling_iteration.png&quot;, dpi = 500, width = 1.1 * 7, height = 1.1 * 4.5) for (m in unique(p_df$model)) { p_df %&gt;% filter(y_label == &quot;Posterior Samples&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2)) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/{m}.png&quot;), width = 5.5, height = 4.5) p_df %&gt;% filter(y_label == &quot;Posterior Samples&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2, col = prob.true)) + scale_color_scico(palette = &quot;berlin&quot;) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/{m}-prob.png&quot;), width = 5.5, height = 4.5) p_df %&gt;% filter(y_label == &quot;True Data&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2, col = prob.true)) + scale_color_scico(palette = &quot;berlin&quot;) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/true-{m}-prob.png&quot;), width = 5.5, height = 4.5) } "],["stan-explained.html", "9 Stan Explained 9.1 Installing stan 9.2 gmm.stan file 9.3 More examples", " 9 Stan Explained 9.1 Installing stan Our suggestion is to use the new cmdstanr 9.2 gmm.stan file Be very careful with the indenting and format data { int&lt;lower=0&gt; K; int&lt;lower=0&gt; N; matrix[N, 2] x; } parameters { simplex[K] pi; matrix[K, 2] mu; real&lt;lower=0&gt; sigma; } model { vector[K] log_prob; for (n in 1:N) { for (k in 1:K) { log_prob[k] = log(pi[k]) + normal_lpdf(x[n] | mu[k], sigma); } target += log_sum_exp(log_prob); } } generated quantities { matrix[N, 2] x_sim; for (n in 1:N) { int z; z = categorical_rng(pi); x_sim[n] = to_row_vector(normal_rng(mu[z], sigma)); } } The Stan file defines a mixture of K Gaussians, with shared sigma, mixture weights pi, and component means mu[k]. The data block specifies K, N, and a 2D matrix x (of size N by 2). In the parameters block, pi is a simplex of size K, mu contains K by 2 means, and sigma is positive. The model computes log_prob[k] for each data point, but yes, since normal_lpdf expects scalar inputs, it sums across dimensions when x[n] and mu[k] are vectors. Thought for a few seconds Here’s what this Stan program does, block by block: 9.2.0.1 1. Data block data { int&lt;lower=0&gt; K; // number of mixture components int&lt;lower=0&gt; N; // number of observations matrix[N, 2] x; // N data points in ℝ² } K: how many Gaussian clusters you want. N: how many 2-dimensional observations. x: your observed data, an N×2 matrix. 9.2.0.2 2. Parameters block parameters { simplex[K] pi; // mixing proportions over K components matrix[K, 2] mu; // 2D mean vector for each component real&lt;lower=0&gt; sigma; // shared (isotropic) standard deviation } pi is a length-K simplex (nonnegative, sums to 1) giving the prior weight of each component. mu[k] is the 2-vector mean of component k. sigma is a single positive scalar: all components share the same spherical (isotropic) standard deviation. 9.2.0.3 3. Model block model { vector[K] log_prob; for (n in 1:N) { for (k in 1:K) { log_prob[k] = log(pi[k]) // log prior weight of component k + normal_lpdf(x[n] | mu[k], sigma); // this expands to the sum of two independent // Normal(μ[k][d], σ) log-densities for d=1,2 } target += log_sum_exp(log_prob); // “log mixture density” trick } } For each data point x[n] and each component k, it computes \\[ \\log\\bigl(\\pi_k\\bigr) + \\sum_{d=1}^2 \\log\\bigl[ \\mathrm{Normal}\\bigl(x_{n,d}\\mid \\mu_{k,d},\\sigma\\bigr) \\bigr]. \\] It then adds \\(\\log\\bigl(\\sum_k \\exp(\\text{that})\\bigr)\\) to the joint log-posterior, which implements a K-component Gaussian mixture likelihood. 9.2.0.4 4. Generated Quantities block generated quantities { matrix[N, 2] x_sim; for (n in 1:N) { int z; z = categorical_rng(pi); // sample a component label x_sim[n] = to_row_vector(normal_rng(mu[z], sigma)); } } After fitting, this simulates a new dataset x_sim of the same size: Draw a component label z from Categorical(pi). Draw a 2-vector from Normal(mu[z], σ) in each coordinate. This gives you posterior-predictive (replicated) data from the inferred mixture. 9.2.1 In plain English You are fitting a K-component mixture of 2D, spherical Gaussians to your data. π are the mixing weights, μₖ are the 2D cluster centers, σ is the common cluster “spread.” The sampler will infer the posterior distribution over these parameters, and then generate a new simulated dataset of size N from that fitted mixture. 9.3 More examples Clustering and Latent Dirichlet "],["all-the-tools-already-available.html", "10 All the tools already available 10.1 A CRAN view of what exists: 10.2 Specific Packages:", " 10 All the tools already available 10.1 A CRAN view of what exists: https://cran.r-project.org/web/views/ExperimentalDesign.html 10.2 Specific Packages: acebayes: Optimal Bayesian Experimental Design using the ACE Algorithm adaptDiag: Bayesian Adaptive Designs for Diagnostic Trials ADCT: Adaptive Design in Clinical Trials adoptr: Adaptive Optimal Two-Stage Designs in R adpss: Design and Analysis of Locally or Globally Efficient Adaptive Designs agricolae: Statistical Procedures for Agricultural Research agricolaeplotr: Visualization of Design of Experiments from the ‘agricolae’ Package AlgDesign: Algorithmic Experimental Design ALTopt: Optimal Experimental Designs for Accelerated Life Testing Aoptbdtvc: A-Optimal Block Designs for Comparing Test Treatments with Controls asd: Simulations for Adaptive Seamless Designs bacistool: Bayesian Classification and Information Sharing (BaCIS) Tool for the Design of Multi-Group Phase II Clinical Trials BayesCR: Bayesian Analysis of Censored Regression Models Under Scale Mixture of Skew Normal Distributions bayesCT: Simulation and Analysis of Adaptive Bayesian Clinical Trials BayesDesign: Bayesian Single-Arm Design with Survival Endpoints BayesPPD: Bayesian Power Prior Design BDEsize: Efficient Determination of Sample Size in Balanced Design of Experiments bdlp: Transparent and Reproducible Artificial Data Generation bioOED: Sensitivity Analysis and Optimum Experiment Design for Microbial Inactivation deepgp: Sequential Design for Deep Gaussian Processes using MCMC DelayedEffect.Design: Sample Size and Power Calculations using the APPLE and SEPPLE Methods demu: Optimal Design Emulators via Point Processes DiceDesign: Designs of Computer Experiments DiceKriging: Kriging Methods for Computer Experiments DiceOptim: Kriging-Based Optimization for Computer Experiments earlyR: Estimation of Transmissibility in the Early Stages of a Disease Outbreak easypower: Sample Size Estimation for Experimental Designs experDesign: Design Experiments for Batches faux: Simulation for Factorial Designs FielDHub: A Shiny App for Design of Experiments in Life Sciences FieldSim: Random Fields (and Bridges) Simulations geospt: Geostatistical Analysis and Design of Optimal Spatial Sampling Networks gscounts: Group Sequential Designs with Negative Binomial Outcomes hiPOD: hierarchical Pooled Optimal Design MCPModPack: Simulation-Based Design and Analysis of Dose-Finding Trials scDesign: Simulation-based scRNA-seq experimental design phyclust: (seqgen) Simulate the evolution of nucleotide or amino acid sequences along a phylogeny splatter: Simulation of single-cell RNA sequencing count data MSstatsSampleSize: Simulation tool for optimal design of high-dimensional MS-based proteomics experiment PROPER: Simulation based methods for evaluating the statistical power in differential expression analysis from RNA-seq data MOSim: Simulates multi-omic experiments that mimic regulatory mechanisms within the cell mbtransfer: Simulation of hypothetical community trajectories under user-specified perturbations. miniLNM :For fitting and using logistic-normal multinomial models. It wraps a simple ‘Stan’ script. multimedia : Multimodal mediation analysis of microbiome data "],["multimodal-data-examples-of-approaches.html", "11 Multimodal data examples of approaches 11.1 Special data structures", " 11 Multimodal data examples of approaches There are some good examples for the microbiome that care instructuve for other domains. Multimedia website install.packages(&quot;multimedia&quot;) ####or you can use the new version: devtools::install_github(&quot;krisrs1128/multimedia&quot;) BiocManager::install(&quot;phyloseq&quot;) 11.1 Special data structures Summarized Experiment About MultiAssayExperiments About phyloseq Lecture on Microbiome Data Computational Challenges of Multidomain Data Complex dependent Factors for Multiway Data "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
