[["index.html", "Generative Modeling Workshop 1 Index", " Generative Modeling Workshop Susan Holmes 1 Index "],["workshop-on-using-generative-models.html", "2 Workshop on using generative models 2.1 Our learning goals: 2.2 Prerequisites:", " 2 Workshop on using generative models Lisbon, 7, 8 May, 2025 2.1 Our learning goals: Learn how to use simulation tools to solve and understand statistical models. Go over examples of experimental design using generative models. Use different R packages to simulate data according to a hierarchical model with multiple levels of random variation. 2.2 Prerequisites: Some statistical know-how and R and RStudio installed on your laptops. Some useful buzzwords and acronyms to look up in case: GOF: Goodness of Fit MCMC: Markov chain Monte Carlo ABC: Approximate Bayesian Computation Random generation: From uniform and non-uniform distributions. Mixtures: Finite combinations of known distributions Hierarchical Models: Infinite mixtures GANs: Generative Adversarial network Identifiability GMM: Gaussian Mixture Models (beware GMM : Generalized Method of Moments) GLM : Generalized Linear Models GLMM: General Linear Mixture Models HMC: Hamiltonian Monte Carlo "],["beyond-black-box-simulation.html", "3 Beyond Black Box Simulation 3.1 New Lingua Franca of Science", " 3 Beyond Black Box Simulation Beyond Black Box Simulation Lisbon Nova [7 | May | 2025] Paper: https://go.wisc.edu/833zs8 Code: https://go.wisc.edu/7222i9 Susan Holmes Stanford University joint work with, Kris Sankaran, UW-Madison 3.1 New Lingua Franca of Science Simulators have emerged as a general problem-solving device across various domains, many of which now have rich, open-source libraries. Where is the interface with statistics? Experimental design, model building, and decision-making. The E3SM is used for long-term climate projections. 3.1.1 New Lingua Franca of Science Simulators have emerged as a general problem-solving device across various domains, many of which now have rich, open-source libraries. Where is the interface with statistics? Experimental design, model building, and decision-making. Splatter generates synthetic single-cell genomics data. 3.1.2 Grammar of Generative Models Transparent simulators can be built by interactively composing simple modules. Probabilistic programming has simplified the process. Regression b: Hierarchy c: Latent Structure d: Temporal Variation 3.1.3 Discrepancy and Iterability By learning a discriminator to contrast real vs. simulated data, we can systematically improve the assumed generative mechanism. 3.1.4 Experimental Design Renaissance Let’s consider a microbiomics case study: To block or not to block? Blocking removes person-level effects… …but increases participant burden. 3.1.5 Simulation to the Rescue How can we navigate trade-offs like this? Simulate! Simulators provide data for more precise decision-making. 3.1.6 Covasim Following the outbreak of COVID-19, the research community came together to build simulators that could inform pandemic response. * E.g., “What would happen if we held classes remotely for two weeks?” 3.1.7 Covasim Covasim is an example of an agent-based model. Starting from local interaction rules, it lets us draw global inferences. Statistical emulators mimic the relationship between input hyperparameters and output data, substantially reducing the computational burden. Inference and imagination: Statistical calibration grounds us in reality while generative tinkering encourages us to imagine. 3.1.8 What are we going to study ? Many examples, like showing non-identifiability, the evolution of mimicry in butterflies, longitudinal study design, the duality between agents and particles, … * Paper Link: https://go.wisc.edu/833zs8 * Code (R + Python + NetLogo): https://go.wisc.edu/7222i9 A github repository of the material is available here: https://spholmes.github.io/Generative_Modeling/ "],["slides.html", "4 Slides", " 4 Slides NotBlackBoxSimulation Experimental Design "],["labs-and-challenges.html", "5 Labs and Challenges :", " 5 Labs and Challenges : Find code for an example of a hierarchical generative model in the book: Modern Statistics for Modern Biology Install the package generative from the github repository spholme/generativeusing the packagedevtools` with devtools::install_github(&quot;spholmes/generative&quot;) Try out examples from the repository of the paper at: https://go.wisc.edu/833zs8 Code: https://go.wisc.edu/7222i9 Install Stan : you can ask a LLM to help for instance chatGPT provided: https://chatgpt.com/share/681bc493-1a64-8005-8d07-cc77cff145d7 Do the examples in the modeling and simulation folders of the generative-review github repo. Download the package mbtransfer and inspect the simulation component of the package (there is a CRAN and a github version). "],["non-identifiability-in-mixture-models-an-example.html", "6 Non-Identifiability in Mixture Models: An Example 6.1 Introduction to Non-Identifiability 6.2 The Binomial Mixture Model 6.3 Simulating Data from Mixture Models 6.4 Demonstrating Non-Identifiability 6.5 Comparing Distributions Directly 6.6 Exploring Parameter Space 6.7 Practical Implications for Inference 6.8 Non-Identifiability as a Function of Separation 6.9 Visualizing the Non-Identifiability Region 6.10 The Likelihood Surface 6.11 Practical Implications and Solutions 6.12 Conclusion", " 6 Non-Identifiability in Mixture Models: An Example 6.1 Introduction to Non-Identifiability Non-identifiability occurs when multiple different parameter values lead to identical (or nearly identical) probability distributions. In such cases, it becomes challenging or impossible to determine the true parameters from observed data, as different parameter combinations can explain the same observations equally well. This tutorial demonstrates this concept using a mixture of two binomial distributions. We’ll show how different combinations of mixture proportions and probability parameters can generate virtually indistinguishable data. 6.2 The Binomial Mixture Model Consider a mixture of two binomial distributions with the following parameters: - Mixture proportion: \\(\\alpha\\) and \\((1-\\alpha)\\) - Success probabilities: \\(p_1\\) and \\(p_2\\) - Number of trials: \\(n\\) The probability mass function is: \\[P(X=k) = \\alpha \\binom{n}{k} p_1^k (1-p_1)^{n-k} + (1-\\alpha) \\binom{n}{k} p_2^k (1-p_2)^{n-k}\\] 6.3 Simulating Data from Mixture Models First, let’s create a function that generates data from a mixture of two binomial distributions: # Function to generate data from a mixture of two binomials generate_binomial_mixture &lt;- function(n_samples, n_trials, alpha, p1, p2) { # Determine which component each sample comes from component &lt;- rbinom(n_samples, 1, alpha) # Generate binomial observations samples &lt;- numeric(n_samples) for (i in 1:n_samples) { if (component[i] == 1) { samples[i] &lt;- rbinom(1, n_trials, p1) } else { samples[i] &lt;- rbinom(1, n_trials, p2) } } return(list( samples = samples, component = component, parameters = list(alpha = alpha, p1 = p1, p2 = p2, n_trials = n_trials) )) } Let’s also define a function to plot the frequency distribution of our data: # Function to plot the histogram of a binomial mixture plot_binomial_mixture &lt;- function(data, title) { # Convert to data frame for ggplot df &lt;- data.frame(value = data$samples) # Extract parameters for the title params &lt;- data$parameters subtitle &lt;- sprintf(&quot;α = %.2f, p₁ = %.2f, p₂ = %.2f, n = %d&quot;, params$alpha, params$p1, params$p2, params$n_trials) # Calculate PMF for theoretical overlay x_values &lt;- 0:params$n_trials pmf_values &lt;- params$alpha * dbinom(x_values, params$n_trials, params$p1) + (1 - params$alpha) * dbinom(x_values, params$n_trials, params$p2) theoretical_df &lt;- data.frame( x = x_values, density = pmf_values ) # Create histogram with theoretical overlay ggplot(df, aes(x = value)) + geom_histogram(aes(y = after_stat(density)), bins = params$n_trials + 1, fill = &quot;skyblue&quot;, color = &quot;white&quot;, alpha = 0.7) + geom_line(data = theoretical_df, aes(x = x, y = density), color = &quot;red&quot;, size = 1) + geom_point(data = theoretical_df, aes(x = x, y = density), color = &quot;red&quot;, size = 3) + labs(title = title, subtitle = subtitle, x = &quot;Number of Successes&quot;, y = &quot;Density&quot;) + theme_minimal() + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 12), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) } 6.4 Demonstrating Non-Identifiability Now, let’s create two different parameter sets that generate similar data distributions: # Set parameters for our two models n_samples &lt;- 10000 n_trials &lt;- 20 # Parameter Set 1 alpha1 &lt;- 0.7 p1_1 &lt;- 0.3 p2_1 &lt;- 0.8 # Parameter Set 2 (different parameters but similar distribution) alpha2 &lt;- 0.5 p1_2 &lt;- 0.2 p2_2 &lt;- 0.7 # Generate data from both models data1 &lt;- generate_binomial_mixture(n_samples, n_trials, alpha1, p1_1, p2_1) data2 &lt;- generate_binomial_mixture(n_samples, n_trials, alpha2, p1_2, p2_2) # Plot histograms of both datasets plot1 &lt;- plot_binomial_mixture(data1, &quot;Binomial Mixture - Parameter Set 1&quot;) plot2 &lt;- plot_binomial_mixture(data2, &quot;Binomial Mixture - Parameter Set 2&quot;) # Display plots side by side grid.arrange(plot1, plot2, ncol = 2) 6.5 Comparing Distributions Directly Let’s compare the theoretical probability mass functions of both parameter sets to better visualize their similarity: # Create a function to compute the theoretical PMF compute_binomial_mixture_pmf &lt;- function(n_trials, alpha, p1, p2) { x &lt;- 0:n_trials pmf &lt;- alpha * dbinom(x, n_trials, p1) + (1 - alpha) * dbinom(x, n_trials, p2) return(data.frame(x = x, density = pmf, parameters = sprintf(&quot;α=%.1f, p₁=%.1f, p₂=%.1f&quot;, alpha, p1, p2))) } # Compute PMFs for both parameter sets pmf1 &lt;- compute_binomial_mixture_pmf(n_trials, alpha1, p1_1, p2_1) pmf2 &lt;- compute_binomial_mixture_pmf(n_trials, alpha2, p1_2, p2_2) # Combine data for plotting combined_pmf &lt;- rbind(pmf1, pmf2) # Create comparison plot ggplot(combined_pmf, aes(x = x, y = density, color = parameters, group = parameters)) + geom_line(size = 1.2) + geom_point(size = 3, alpha = 0.7) + labs(title = &quot;Comparison of Theoretical Probability Mass Functions&quot;, subtitle = &quot;Different parameter sets producing similar distributions&quot;, x = &quot;Number of Successes&quot;, y = &quot;Probability&quot;, color = &quot;Parameter Set&quot;) + theme_minimal() + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 12), axis.title = element_text(size = 12), axis.text = element_text(size = 10), legend.position = &quot;bottom&quot;, legend.title = element_text(size = 12), legend.text = element_text(size = 10) ) 6.6 Exploring Parameter Space Let’s explore a wider range of parameter combinations to better understand non-identifiability in this model: # Create a grid of different parameter combinations param_grid &lt;- expand.grid( alpha = c(0.3, 0.5, 0.7), p1 = c(0.2, 0.3, 0.4), p2 = c(0.6, 0.7, 0.8) ) # Compute PMF for each parameter combination pmf_list &lt;- list() for (i in 1:nrow(param_grid)) { pmf_list[[i]] &lt;- compute_binomial_mixture_pmf( n_trials, param_grid$alpha[i], param_grid$p1[i], param_grid$p2[i] ) } # Combine all PMFs all_pmfs &lt;- do.call(rbind, pmf_list) # Plot all PMFs ggplot(all_pmfs, aes(x = x, y = density, color = parameters, group = parameters)) + geom_line(size = 1, alpha = 0.7) + labs(title = &quot;Multiple Parameter Combinations of Binomial Mixtures&quot;, subtitle = &quot;Illustrating parameter redundancy and non-identifiability&quot;, x = &quot;Number of Successes&quot;, y = &quot;Probability&quot;, color = &quot;Parameters&quot;) + theme_minimal() + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 12), axis.title = element_text(size = 12), axis.text = element_text(size = 10), legend.position = &quot;right&quot;, legend.text = element_text(size = 8) ) 6.7 Practical Implications for Inference To demonstrate the challenge of parameter recovery, let’s try to infer the parameters from simulated data using a simple method of moments approach: # Function to calculate the first three moments of a binomial mixture calc_moments &lt;- function(n_trials, alpha, p1, p2) { # Mean m1 &lt;- n_trials * (alpha * p1 + (1 - alpha) * p2) # Second central moment (variance) var1 &lt;- n_trials * alpha * p1 * (1 - p1) var2 &lt;- n_trials * (1 - alpha) * p2 * (1 - p2) mixture_var &lt;- var1 + var2 + n_trials^2 * alpha * (1 - alpha) * (p1 - p2)^2 m2 &lt;- mixture_var # Third central moment (skewness related) m3 &lt;- alpha * (p1 * (1 - p1) * (1 - 2*p1)) + (1 - alpha) * (p2 * (1 - p2) * (1 - 2*p2)) m3 &lt;- m3 * n_trials + 3 * n_trials * (n_trials - 1) * (alpha * p1^2 * (1 - p1) + (1 - alpha) * p2^2 * (1 - p2)) + n_trials * (n_trials - 1) * (n_trials - 2) * (alpha * p1^3 + (1 - alpha) * p2^3) return(c(m1, m2, m3)) } # Function to compute the distance between two sets of moments moment_distance &lt;- function(params, observed_moments, n_trials) { alpha &lt;- params[1] p1 &lt;- params[2] p2 &lt;- params[3] # Ensure parameters are within valid range if (alpha &lt; 0 || alpha &gt; 1 || p1 &lt; 0 || p1 &gt; 1 || p2 &lt; 0 || p2 &gt; 1) { return(Inf) } theoretical_moments &lt;- calc_moments(n_trials, alpha, p1, p2) # Calculate sum of squared differences between moments return(sum((theoretical_moments - observed_moments)^2)) } # Simulate data with known parameters true_alpha &lt;- 0.6 true_p1 &lt;- 0.3 true_p2 &lt;- 0.8 n_trials &lt;- 20 n_samples &lt;- 1000 simulated_data &lt;- generate_binomial_mixture(n_samples, n_trials, true_alpha, true_p1, true_p2) # Calculate empirical moments from the data emp_mean &lt;- mean(simulated_data$samples) emp_var &lt;- var(simulated_data$samples) emp_m3 &lt;- mean((simulated_data$samples - emp_mean)^3) # Third central moment observed_moments &lt;- c(emp_mean, emp_var, emp_m3) # Create a grid of parameter values to evaluate moments alpha_grid &lt;- seq(0.1, 0.9, by = 0.1) p1_grid &lt;- seq(0.1, 0.9, by = 0.1) p2_grid &lt;- seq(0.1, 0.9, by = 0.1) # Create a data frame to store results results &lt;- expand.grid(alpha = alpha_grid, p1 = p1_grid, p2 = p2_grid) results$distance &lt;- NA # Calculate moment distance for each parameter combination for (i in 1:nrow(results)) { results$distance[i] &lt;- moment_distance( c(results$alpha[i], results$p1[i], results$p2[i]), observed_moments, n_trials ) } # Filter out invalid combinations (where p1 ≥ p2) results &lt;- results %&gt;% filter(p1 &lt; p2) %&gt;% filter(is.finite(distance)) %&gt;% # Scale distances for better visualization mutate(relative_distance = distance / max(distance)) # Find the top 5 best fitting parameter combinations # Find the top 5 best fitting parameter combinations top_fits &lt;- results %&gt;% arrange(distance) %&gt;% head(5) # Show true parameters vs estimated parameters true_params &lt;- data.frame( alpha = true_alpha, p1 = true_p1, p2 = true_p2, distance = 0, # Change from NA to 0 relative_distance = 0, # Change from NA to 0 type = &quot;True Parameters&quot; ) top_fits$type &lt;- &quot;Estimated Parameters&quot; comparison &lt;- rbind(true_params, top_fits) # Print comparison table knitr::kable(comparison, caption = &quot;True vs. Estimated Parameters&quot;, digits = 3) Table 6.1: True vs. Estimated Parameters alpha p1 p2 distance relative_distance type 0.6 0.3 0.8 0.000 0 True Parameters 0.6 0.1 0.2 667.831 0 Estimated Parameters 0.5 0.1 0.2 675.903 0 Estimated Parameters 0.9 0.1 0.3 728.359 0 Estimated Parameters 0.7 0.1 0.2 775.146 0 Estimated Parameters 0.4 0.1 0.2 799.275 0 Estimated Parameters # Create visualization of parameter space ggplot(results %&gt;% filter(relative_distance &lt; 0.1), aes(x = p1, y = p2, color = alpha, size = 1-relative_distance)) + geom_point(alpha = 0.7) + scale_size_continuous(range = c(1, 5)) + scale_color_viridis_c() + labs(title = &quot;Parameter Combinations with Similar Fit to Data&quot;, subtitle = &quot;Points show parameter combinations with low moment distance&quot;, x = &quot;p₁&quot;, y = &quot;p₂&quot;, color = &quot;α&quot;, size = &quot;Goodness of fit&quot;) + geom_point(data = true_params, aes(x = p1, y = p2), color = &quot;red&quot;, size = 8, shape = 4) + annotate(&quot;text&quot;, x = true_p1, y = true_p2 + 0.05, label = &quot;True parameters\\n(distance = 0)&quot;, color = &quot;red&quot;, fontface = &quot;bold&quot;) + theme_minimal() + theme(legend.position = &quot;right&quot;) 6.8 Non-Identifiability as a Function of Separation Let’s examine how the degree of non-identifiability depends on the separation between the component distributions. Intuitively, we might expect that when the two binomial components have very different success probabilities, it becomes easier to identify the true parameters. # Examine how the set of alternative parameters changes with the separation between p₁ and p₂ # Define a sequence of increasing separations between p₁ and p₂ separation_sequence &lt;- seq(0.1, 0.8, by = 0.1) n &lt;- 10 # Number of trials in binomial # Create a plot to visualize how the set of alternative parameters changes with separation par(mfrow = c(2, 4), mar = c(4, 4, 2, 1)) for (sep in separation_sequence) { # Define true parameters with increasing separation p1_true &lt;- 0.1 p2_true &lt;- p1_true + sep lambda_true &lt;- 0.5 # Create a grid of alternative parameter values p1_alt &lt;- seq(0.01, 0.99, length.out = 100) p2_alt &lt;- seq(0.01, 0.99, length.out = 100) grid &lt;- expand.grid(p1 = p1_alt, p2 = p2_alt) # Calculate PMF for the true parameters pmf_true &lt;- rep(0, n+1) for (k in 0:n) { pmf_true[k+1] &lt;- lambda_true * dbinom(k, n, p1_true) + (1 - lambda_true) * dbinom(k, n, p2_true) } # Calculate KL divergence for each grid point kl_div &lt;- numeric(nrow(grid)) for (i in 1:nrow(grid)) { # For each alternative (p1, p2) pair, find the lambda that minimizes KL divergence minimize_kl &lt;- function(lambda) { pmf_alt &lt;- rep(0, n+1) for (k in 0:n) { pmf_alt[k+1] &lt;- lambda * dbinom(k, n, grid$p1[i]) + (1 - lambda) * dbinom(k, n, grid$p2[i]) } # Avoid log(0) issues pmf_alt &lt;- pmax(pmf_alt, 1e-10) pmf_true_safe &lt;- pmax(pmf_true, 1e-10) # Calculate KL divergence kl &lt;- sum(pmf_true_safe * log(pmf_true_safe / pmf_alt)) return(kl) } # Find optimal lambda using a grid search (for simplicity) lambda_grid &lt;- seq(0.01, 0.99, by = 0.01) kl_values &lt;- sapply(lambda_grid, minimize_kl) best_lambda_idx &lt;- which.min(kl_values) kl_div[i] &lt;- kl_values[best_lambda_idx] } # Convert to matrix for contour plot kl_matrix &lt;- matrix(kl_div, nrow = length(p1_alt), ncol = length(p2_alt)) # Plot contours of KL divergence contour_levels &lt;- c(0.001, 0.005, 0.01, 0.05) plot(0, 0, type = &quot;n&quot;, xlim = c(0, 1), ylim = c(0, 1), xlab = &quot;p₁&quot;, ylab = &quot;p₂&quot;, main = paste(&quot;Separation = &quot;, sep)) # Add contour lines contour(p1_alt, p2_alt, kl_matrix, levels = contour_levels, add = TRUE, drawlabels = TRUE, col = &quot;blue&quot;) # Mark the true parameters points(p1_true, p2_true, pch = 16, col = &quot;red&quot;) # Add a line for p1 = p2 abline(0, 1, lty = 2) } # Reset plot parameters par(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1) In the plot above, we can see that as the separation between p₁ and p₂ increases, the set of alternative parameters that generate similar distributions becomes more constrained. This demonstrates that non-identifiability is most pronounced when the component distributions are similar to each other. 6.9 Visualizing the Non-Identifiability Region Let’s create a more detailed visualization of the parameter space that shows which combinations of parameters lead to nearly identical distributions: # Define a function to compute distance between distributions compute_dist_between_distributions &lt;- function(n_trials, alpha1, p1_1, p2_1, alpha2, p1_2, p2_2) { # Compute PMFs x &lt;- 0:n_trials pmf1 &lt;- alpha1 * dbinom(x, n_trials, p1_1) + (1 - alpha1) * dbinom(x, n_trials, p2_1) pmf2 &lt;- alpha2 * dbinom(x, n_trials, p1_2) + (1 - alpha2) * dbinom(x, n_trials, p2_2) # Compute total variation distance return(0.5 * sum(abs(pmf1 - pmf2))) } # Fix one parameter set fixed_alpha &lt;- 0.7 fixed_p1 &lt;- 0.2 fixed_p2 &lt;- 0.6 n_trials &lt;- 20 # Create grid of alternative parameters alpha_alt &lt;- seq(0.1, 0.9, by = 0.05) p1_alt &lt;- seq(0.1, 0.5, by = 0.02) p2_alt &lt;- seq(0.4, 0.9, by = 0.02) # Create grid of all combinations param_space &lt;- expand.grid(alpha = alpha_alt, p1 = p1_alt, p2 = p2_alt) # Filter to ensure p1 &lt; p2 param_space &lt;- param_space %&gt;% filter(p1 &lt; p2) # Calculate distance for each parameter combination param_space$distance &lt;- mapply( function(a, p1, p2) { compute_dist_between_distributions(n_trials, fixed_alpha, fixed_p1, fixed_p2, a, p1, p2) }, param_space$alpha, param_space$p1, param_space$p2 ) # Set a threshold for &quot;nearly identical&quot; distributions threshold &lt;- 0.05 param_space$nearly_identical &lt;- param_space$distance &lt; threshold # Select more informative alpha slices (avoiding exact α = 0.7) alpha_slices &lt;- c(0.3, 0.5, 0.65) sliced_data &lt;- param_space %&gt;% filter(alpha %in% alpha_slices) # Create reference point data frame reference_point &lt;- data.frame( p1 = fixed_p1, p2 = fixed_p2, alpha = fixed_alpha, distance = 0, nearly_identical = TRUE ) # Set a maximum distance for better visualization max_dist_for_scale &lt;- quantile(param_space$distance, 0.9) # Create heatmap for each alpha slice ggplot(sliced_data, aes(x = p1, y = p2, fill = distance)) + geom_tile() + facet_wrap(~alpha, labeller = labeller(alpha = function(x) paste(&quot;α =&quot;, x))) + scale_fill_viridis_c(option = &quot;plasma&quot;, direction = -1, limits = c(0, max_dist_for_scale), oob = scales::squish) + geom_point(data = reference_point %&gt;% filter(alpha %in% c(fixed_alpha)), color = &quot;red&quot;, size = 3, shape = 4, show.legend = FALSE) + annotate(&quot;text&quot;, x = 0.15, y = 0.85, label = paste(&quot;Reference:&quot;, fixed_alpha, fixed_p1, fixed_p2), color = &quot;red&quot;, size = 3, hjust = 0) + labs( title = &quot;Non-Identifiability Regions in Parameter Space&quot;, subtitle = paste0(&quot;Distance from reference distribution (α = &quot;, fixed_alpha, &quot;, p₁ = &quot;, fixed_p1, &quot;, p₂ = &quot;, fixed_p2, &quot;)&quot;), x = &quot;p₁&quot;, y = &quot;p₂&quot;, fill = &quot;Distribution\\nDistance&quot; ) + theme_minimal() + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), strip.background = element_rect(fill = &quot;lightblue&quot;, color = NA), strip.text = element_text(face = &quot;bold&quot;) ) The heatmap above shows slices of the parameter space at different values of α. The darker regions represent parameter combinations that produce distributions very similar to our reference distribution. The red × marks the location of the reference parameters. 6.10 The Likelihood Surface Another way to visualize non-identifiability is through the likelihood surface. For a given dataset, multiple parameter combinations can achieve similarly high likelihood values: Another way to visualize non-identifiability is through the likelihood surface. For a given dataset, multiple parameter combinations can achieve similarly high likelihood values: Let’s create a more detailed visualization of the parameter space that shows which combinations of parameters lead to nearly identical distributions: # Generate a dataset from our reference parameters set.seed(123) reference_data &lt;- generate_binomial_mixture(1000, n_trials, fixed_alpha, fixed_p1, fixed_p2) # Function to compute log-likelihood for binomial mixture compute_log_likelihood &lt;- function(data, n_trials, alpha, p1, p2) { x &lt;- data$samples pmf &lt;- alpha * dbinom(x, n_trials, p1) + (1 - alpha) * dbinom(x, n_trials, p2) return(sum(log(pmf))) } # Calculate log-likelihood for our parameter grid param_space$log_likelihood &lt;- mapply( function(a, p1, p2) { compute_log_likelihood(reference_data, n_trials, a, p1, p2) }, param_space$alpha, param_space$p1, param_space$p2 ) # Normalize log-likelihood for better visualization param_space$rel_likelihood &lt;- param_space$log_likelihood - max(param_space$log_likelihood) # Select alpha values close to but not exactly at the reference alpha_ll_slices &lt;- c(0.6, 0.65, 0.75) sliced_ll_data &lt;- param_space %&gt;% filter(alpha %in% alpha_ll_slices) # Calculate the log-likelihood for the reference parameters ref_ll &lt;- compute_log_likelihood(reference_data, n_trials, fixed_alpha, fixed_p1, fixed_p2) ref_rel_ll &lt;- ref_ll - max(param_space$log_likelihood) # Create a separate data frame for the reference point reference_point_for_plot &lt;- data.frame( p1 = fixed_p1, p2 = fixed_p2, alpha = fixed_alpha, log_likelihood = ref_ll, rel_likelihood = ref_rel_ll ) # Find a suitable minimum for the likelihood scale min_ll_for_scale &lt;- quantile(sliced_ll_data$rel_likelihood, 0.1) # Plot likelihood surface ll_plot &lt;- ggplot(sliced_ll_data, aes(x = p1, y = p2, fill = rel_likelihood)) + geom_tile() + facet_wrap(~alpha, labeller = labeller(alpha = function(x) paste(&quot;α =&quot;, x))) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;darkblue&quot;, limits = c(min_ll_for_scale, 0), oob = scales::squish) + annotate(&quot;text&quot;, x = 0.15, y = 0.85, label = paste(&quot;Reference:&quot;, fixed_alpha, fixed_p1, fixed_p2), color = &quot;red&quot;, size = 3, hjust = 0) + labs( title = &quot;Log-Likelihood Surface for Binomial Mixture&quot;, subtitle = &quot;Showing regions of high likelihood (dark blue) across parameter space&quot;, x = &quot;p₁&quot;, y = &quot;p₂&quot;, fill = &quot;Relative\\nLog-Likelihood&quot; ) + theme_minimal() + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), strip.background = element_rect(fill = &quot;lightblue&quot;, color = NA), strip.text = element_text(face = &quot;bold&quot;) ) # Display the plot ll_plot The likelihood surface shows a ridge of high-likelihood values rather than a single clear peak, which is characteristic of non-identifiable models. This ridge corresponds to different parameter combinations that explain the data equally well. 6.11 Practical Implications and Solutions The non-identifiability of mixture models has important practical implications: Parameter Interpretation: In non-identifiable models, individual parameter estimates may not be reliable or interpretable, even with large sample sizes. Uncertainty Quantification: Standard errors and confidence intervals can be misleading as they may not capture the true uncertainty. Model Selection: Simpler models with fewer parameters might be preferred even if they don’t fit the data as well. Some approaches to dealing with non-identifiability include: Constraints: Imposing constraints on parameters (e.g., ordering the components by their means). Prior Information: Using Bayesian methods with informative priors. Alternative Parameterizations: Finding alternative ways to parameterize the model that are identifiable. Let’s demonstrate how adding constraints can help: # Analyze identifiability with constraints # Here we&#39;ll constrain p1 &lt; p2 and alpha &gt; 0.5 # Filter our parameter space accordingly constrained_space &lt;- param_space %&gt;% filter(p1 &lt; p2, alpha &gt; 0.5) # Visualize the constrained parameter space using better alpha values constrained_alpha_slices &lt;- c(0.6, 0.65, 0.75) constrained_ll_data &lt;- constrained_space %&gt;% filter(alpha %in% constrained_alpha_slices) # Find a suitable minimum for the constrained likelihood scale min_constrained_ll &lt;- quantile(constrained_ll_data$rel_likelihood, 0.1) # Plot constrained likelihood surface constrained_plot &lt;- ggplot(constrained_ll_data, aes(x = p1, y = p2, fill = rel_likelihood)) + geom_tile() + facet_wrap(~alpha, labeller = labeller(alpha = function(x) paste(&quot;α =&quot;, x))) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;darkblue&quot;, limits = c(min_constrained_ll, 0), oob = scales::squish) + annotate(&quot;text&quot;, x = 0.15, y = 0.85, label = paste(&quot;Reference:&quot;, fixed_alpha, fixed_p1, fixed_p2), color = &quot;red&quot;, size = 3, hjust = 0) + labs( title = &quot;Constrained Log-Likelihood Surface&quot;, subtitle = &quot;Adding constraints (p₁ &lt; p₂, α &gt; 0.5) reduces parameter uncertainty&quot;, x = &quot;p₁&quot;, y = &quot;p₂&quot;, fill = &quot;Relative\\nLog-Likelihood&quot; ) + theme_minimal() + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), strip.background = element_rect(fill = &quot;lightblue&quot;, color = NA), strip.text = element_text(face = &quot;bold&quot;) ) # Display the plot constrained_plot 6.12 Conclusion This tutorial has demonstrated the concept of non-identifiability in mixture models, specifically using a mixture of two binomial distributions. We have shown that: Different parameter combinations can produce nearly identical probability distributions. The degree of non-identifiability depends on the separation between mixture components. The likelihood surface often shows ridges rather than clear peaks, reflecting parameter redundancy. Adding constraints can help reduce but not eliminate the problem. When working with mixture models, it’s important to be aware of potential non-identifiability issues and interpret parameter estimates with appropriate caution. In many cases, the focus should be on the overall distribution and model predictions rather than specific parameter values. Understanding non-identifiability is crucial for proper statistical inference, experimental design, and interpretation of results in many areas of science and engineering where mixture models are commonly used. This completes the R Markdown file with a thorough exploration of non-identifiability in binomial mixture models, including the previously missing section on how separation between components affects identifiability. The document includes: 1. Clear explanation of non-identifiability 2. Visualizations of similarly distributed mixtures with different parameters 3. Analysis of how component separation affects identifiability 4. Examination of non-identifiability regions in parameter space 5. Visualization of the likelihood surface 6. Discussion of practical implications and potential solutions The code provides interactive simulations and visualizations that clearly demonstrate how different parameter combinations can lead to nearly identical observed distributions, making it challenging to recover the true parameters from data. "],["intervention.html", "7 Intervention 7.1 Start defining the intervention in a time series", " 7 Intervention 7.1 Start defining the intervention in a time series library(tidyverse) library(splines) library(generative) set.seed(123) times &lt;- seq(-10, 10, length.out = 200) pf &lt;- peak_fun(times, 1, 10, 5) plot(times, pf(times), type = &quot;l&quot;) u &lt;- runif(20, -10, 10) points(u, gaussian_noise(pf, u)) u &lt;- seq(-10, 10, length.out = 10) intervals &lt;- cbind(head(u, -1), tail(u, -1)) weights &lt;- 5:1 weights &lt;- weights / sum(weights) locs &lt;- weighted_sampling(1e4, intervals, weights) hist(locs) We now have a way of simulating parameterized mean functions and observations with noise around them We also have ways of sampling locations to observe the function, based on a weighted combination of uniforms We need a way of estimating these mean functions and evaluating the resulting fit. We also need to evaluate many combinations of weightings, either using random search or bayesian optimization First, we attempt to estimate these mean functions using simple piecewise linear splines w &lt;- c(1, 1, 1, 1, 3, 4, 3, 1, 1) w &lt;- w / sum(w) u &lt;- weighted_sampling(40, intervals, w) y &lt;- gaussian_noise(pf, u) fit &lt;- lm(y ~ bs(u, df = 6, degree = 1, Boundary.knots = c(-10, 10))) plot(times, pf(times), type = &quot;l&quot;) points(u, y, col = &quot;red&quot;) points(u, predict(fit), col = &quot;blue&quot;) f_hat &lt;- predict(fit, newdata = data.frame(u = times)) points(times, f_hat, col = &quot;blue&quot;) B &lt;- 1e3 mse &lt;- matrix(nrow = B, ncol = 2) for (b in seq_len(B)) { # random u&#39;s u &lt;- runif(50, -10, 10) mse[b, 1] &lt;- spline_mse(u, times, pf) u &lt;- weighted_sampling(50, intervals, w) mse[b, 2] &lt;- spline_mse(u, times, pf) } colnames(mse) &lt;- c(&quot;uniform&quot;, &quot;weighted&quot;) mse &lt;- as_tibble(mse) %&gt;% mutate(id = 1:n()) %&gt;% pivot_longer(-id, names_to = &quot;sampling&quot;, values_to = &quot;mse&quot;) ggplot(mse) + geom_histogram( aes(mse, fill = sampling), alpha = 0.8, position = &quot;identity&quot;, bins = 50 ) + scale_y_continuous(expand = c(0, 0, 0.1, 0)) + scale_x_continuous(expand = c(0, 0)) "],["intervention_optimize.html", "8 Intervention_Optimize", " 8 Intervention_Optimize (this can be slow) library(tidyverse) library(laGP) library(purrr) library(generative) library(splines) review_theme() set.seed(123) ### This is very slow n_init &lt;- 100 n_interval &lt;- 7 intervals &lt;- make_intervals(n_interval) times &lt;- seq(-10, 10, length.out = 200) pf &lt;- peak_fun(times, 1, 10, 5) # initial sample of weights betas &lt;- rerun(n_init, rnorm(n_interval, 0, 1)) losses &lt;- map_dbl(betas, ~ evaluate_weights(., intervals, times, pf, N = 25)) We compute the main Bayesian optimization loop below. n_test &lt;- 500 n_add &lt;- 10 n_iters &lt;- 50 for (i in seq_len(n_iters)) { # fit a GP to the betas / losses already computed beta_norm &lt;- scale_list(betas) beta_test &lt;- rerun(n_test, runif(n_interval, -4, 4)) # find most promising new test points y_hat &lt;- aGP(beta_norm, losses, scale_list(beta_test), verb = 0) sort_ix &lt;- order(y_hat$mean - y_hat$var) # evaluate loss on new test points new_betas &lt;- beta_test[sort_ix][1:n_add] betas &lt;- c(betas, new_betas) losses &lt;- c( losses, map_dbl(new_betas, ~ evaluate_weights(., intervals, times, pf, N = 25)) ) } Here are some of the weightings that we should use for longitudinal sampling. They generally favor more samples in the region where the peak is located. w &lt;- bind_rows(map(betas, ~ data.frame(exp(t(.))))) w &lt;- w / rowSums(w) w$loss &lt;- losses library(embed) library(tidymodels) w_reduce &lt;- recipe(~ ., data = w) %&gt;% update_role(loss, new_role = &quot;id&quot;) umap_rec &lt;- step_umap(w_reduce, all_predictors(), neighbors = 30, min_dist = 0.05) scores &lt;- prep(umap_rec) %&gt;% juice() %&gt;% bind_cols(w %&gt;% select(-loss)) ggplot(scores) + geom_point(aes(UMAP1, UMAP2, col = log(loss), size = X4)) + scale_color_viridis_c(direction = -1, limits = c(0, 8), option = &quot;inferno&quot;) w_df &lt;- w %&gt;% mutate(id = row_number()) %&gt;% arrange(-loss) %&gt;% pivot_longer(starts_with(&quot;X&quot;), names_to = &quot;component&quot;) intervals_df &lt;- as.data.frame(intervals) intervals_df$component &lt;- paste0(&quot;X&quot;, 1:nrow(intervals)) w_df &lt;- w_df %&gt;% left_join(intervals_df) ggplot(w_df %&gt;% filter(log(loss) &lt; 0.08)) + geom_rect(aes(xmin = V1, xmax = V2, ymin = 0, ymax = value), fill = &quot;#262526&quot;) + scale_x_continuous(expand = c(0, 0), breaks = c(-5, 0, 5)) + scale_y_continuous(expand = c(0, 0, 0.1, 0)) + facet_grid(. ~ round(loss, 4)) ggsave(&quot;~/Downloads/best_weights.png&quot;) u &lt;- map(1:8, ~ runif(25, -10, 10)) examples &lt;- map_dfr(u, ~ data.frame( u = ., y = gaussian_noise(pf, .) ), .id = &quot;run&quot; ) %&gt;% mutate(run = as.factor(run)) preds &lt;- examples %&gt;% split(.$run) %&gt;% map_dfr(~ { fit &lt;- lm(y ~ bs(u, df = 6, degree = 1, Boundary.knots = c(-10, 10)), data = .) data.frame( times = times, mu = pf(times), f_hat = predict(fit, newdata = data.frame(u = times)) ) }, .id = &quot;run&quot; ) %&gt;% group_by(run) %&gt;% mutate( run = as.factor(run), loss = (f_hat - mu) ^ 2 ) truth &lt;- data.frame(times = times, mu = pf(times)) run_order &lt;- preds %&gt;% group_by(run) %&gt;% summarise(mse = mean(loss)) %&gt;% arrange(mse) %&gt;% pull(run) preds &lt;- preds %&gt;% mutate(run = factor(run, levels = run_order)) examples &lt;- examples %&gt;% mutate(run = factor(run, levels = run_order)) ggplot(preds) + geom_line(aes(times, mu), col = &quot;#F27507&quot;, linewidth = 2) + geom_line(aes(times, f_hat), col = &quot;#087F8C&quot;, linewidth = 1.5) + geom_point(data = examples, aes(u, y), col = &quot;#221F26&quot;) + labs(y = &quot;y&quot;, x = &quot;time&quot;) + facet_wrap(~ run, ncol = 4) ggsave(&quot;~/Downloads/peak_samples.png&quot;, dpi = 800) "],["heteroskedastic-mixtures-goodness-of-fit-iterations.html", "9 Heteroskedastic Mixtures : goodness of fit iterations", " 9 Heteroskedastic Mixtures : goodness of fit iterations library(tidyverse) library(caret) library(scico) library(generative) review_theme() set.seed(090322) Simulate data from a heteroskedastic mixture of t’s Fit a standard GMM (stan) Fit a discriminator (caret) Visualize discriminator probabilities Fit updated heteroskedastic model Repeat discrimination Fit true model Repeat discrimination disc_data &lt;- list() p &lt;- list() x &lt;- t_mixture(500) ggplot(x) + geom_point(aes(V1, V2)) + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) + coord_fixed() ggsave(&quot;~/Downloads/true_mixture.png&quot;, dpi = 400, width = 5) fit &lt;- fit_gmm(x, K = 4) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.000747 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 7.47 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2745.118 1.000 1.000 ## 200 -2427.204 0.565 1.000 ## 300 -2427.541 0.377 0.131 ## 400 -2425.820 0.283 0.131 ## 500 -2425.803 0.226 0.001 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 1.8 seconds. x_sim &lt;- mixture_predictive(fit) ggplot(x_sim) + geom_point(aes(`1`, `2`), size = 0.5, alpha = 0.8) + facet_wrap(~ .iteration) + coord_fixed() disc_data[[&quot;gmm&quot;]] &lt;- bind_dicriminative(x, x_sim) gbmGrid &lt;- expand.grid( interaction.depth = 5, n.trees = 100, shrinkage = 0.1, n.minobsinnode = 2 ) trControl &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 10 ) fit &lt;- train( x = disc_data[[&quot;gmm&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;gmm&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.543 0.086 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was ## held constant at a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning ## parameter &#39;n.minobsinnode&#39; was held constant at a value of 2 p[[&quot;gmm&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;gmm&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;gmm&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) fit &lt;- fit_gmm_cov(x, K = 5) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.002102 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 21.02 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2451.708 1.000 1.000 ## 200 -2445.865 0.501 1.000 ## 300 -2445.244 0.334 0.002 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 3.7 seconds. x_sim &lt;- mixture_predictive(fit) disc_data[[&quot;gmm_cov&quot;]] &lt;- bind_dicriminative(x, x_sim) fit &lt;- train( x = disc_data[[&quot;gmm_cov&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;gmm_cov&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.5253 0.0506 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was ## held constant at a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning ## parameter &#39;n.minobsinnode&#39; was held constant at a value of 2 p[[&quot;gmm_cov&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;gmm_cov&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;gmm_cov&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) fit &lt;- fit_tmm(x, K = 5) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.001971 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 19.71 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2655.391 1.000 1.000 ## 200 -2649.922 0.501 1.000 ## 300 -2637.090 0.336 0.005 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 3.4 seconds. x_sim &lt;- mixture_predictive(fit) ggplot(x_sim) + geom_point(aes(`1`, `2`), size = 0.5, alpha = 0.8) + facet_wrap(~ .iteration) + coord_fixed() disc_data[[&quot;tmm&quot;]] &lt;- bind_dicriminative(x, x_sim, iter = 4) fit &lt;- train( x = disc_data[[&quot;tmm&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;tmm&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.5844 0.1688 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was ## held constant at a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning ## parameter &#39;n.minobsinnode&#39; was held constant at a value of 2 p[[&quot;tmm&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;tmm&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;tmm&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) p_df &lt;- bind_rows(p, .id = &quot;model&quot;) %&gt;% mutate( model = fct_recode(model, &quot;Gaussian (Shared Covariance)&quot; = &quot;gmm&quot;, &quot;Gaussian (Individual Covariance)&quot; = &quot;gmm_cov&quot;, &quot;Student&#39;s t (Individual Covariance)&quot; = &quot;tmm&quot;), y_label = factor(y_label, levels = c(&quot;True Data&quot;, &quot;Posterior Samples&quot;)) ) ggplot(p_df, aes(V1, V2, col = prob.true)) + geom_point(size = 1.2) + scale_color_scico(palette = &quot;berlin&quot;) + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;, col = &quot;Predicted Probability&quot;) + xlim(-11, 11) + coord_fixed() + facet_grid(y_label ~ model) ggsave(&quot;~/Downloads/modeling_iteration.png&quot;, dpi = 500, width = 1.1 * 7, height = 1.1 * 4.5) for (m in unique(p_df$model)) { p_df %&gt;% filter(y_label == &quot;Posterior Samples&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2)) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/{m}.png&quot;), width = 5.5, height = 4.5) p_df %&gt;% filter(y_label == &quot;Posterior Samples&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2, col = prob.true)) + scale_color_scico(palette = &quot;berlin&quot;) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/{m}-prob.png&quot;), width = 5.5, height = 4.5) p_df %&gt;% filter(y_label == &quot;True Data&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2, col = prob.true)) + scale_color_scico(palette = &quot;berlin&quot;) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/true-{m}-prob.png&quot;), width = 5.5, height = 4.5) } "],["stan-explained.html", "10 Stan Explained 10.1 Installing stan 10.2 gmm.stan file 10.3 More examples", " 10 Stan Explained 10.1 Installing stan Our suggestion is to use the new cmdstanr 10.2 gmm.stan file Be very careful with the indenting and format data { int&lt;lower=0&gt; K; int&lt;lower=0&gt; N; matrix[N, 2] x; } parameters { simplex[K] pi; matrix[K, 2] mu; real&lt;lower=0&gt; sigma; } model { vector[K] log_prob; for (n in 1:N) { for (k in 1:K) { log_prob[k] = log(pi[k]) + normal_lpdf(x[n] | mu[k], sigma); } target += log_sum_exp(log_prob); } } generated quantities { matrix[N, 2] x_sim; for (n in 1:N) { int z; z = categorical_rng(pi); x_sim[n] = to_row_vector(normal_rng(mu[z], sigma)); } } The Stan file defines a mixture of K Gaussians, with shared sigma, mixture weights pi, and component means mu[k]. The data block specifies K, N, and a 2D matrix x (of size N by 2). In the parameters block, pi is a simplex of size K, mu contains K by 2 means, and sigma is positive. The model computes log_prob[k] for each data point, but yes, since normal_lpdf expects scalar inputs, it sums across dimensions when x[n] and mu[k] are vectors. Thought for a few seconds Here’s what this Stan program does, block by block: 10.2.0.1 1. Data block data { int&lt;lower=0&gt; K; // number of mixture components int&lt;lower=0&gt; N; // number of observations matrix[N, 2] x; // N data points in ℝ² } K: how many Gaussian clusters you want. N: how many 2-dimensional observations. x: your observed data, an N×2 matrix. 10.2.0.2 2. Parameters block parameters { simplex[K] pi; // mixing proportions over K components matrix[K, 2] mu; // 2D mean vector for each component real&lt;lower=0&gt; sigma; // shared (isotropic) standard deviation } pi is a length-K simplex (nonnegative, sums to 1) giving the prior weight of each component. mu[k] is the 2-vector mean of component k. sigma is a single positive scalar: all components share the same spherical (isotropic) standard deviation. 10.2.0.3 3. Model block model { vector[K] log_prob; for (n in 1:N) { for (k in 1:K) { log_prob[k] = log(pi[k]) // log prior weight of component k + normal_lpdf(x[n] | mu[k], sigma); // this expands to the sum of two independent // Normal(μ[k][d], σ) log-densities for d=1,2 } target += log_sum_exp(log_prob); // “log mixture density” trick } } For each data point x[n] and each component k, it computes \\[ \\log\\bigl(\\pi_k\\bigr) + \\sum_{d=1}^2 \\log\\bigl[ \\mathrm{Normal}\\bigl(x_{n,d}\\mid \\mu_{k,d},\\sigma\\bigr) \\bigr]. \\] It then adds \\(\\log\\bigl(\\sum_k \\exp(\\text{that})\\bigr)\\) to the joint log-posterior, which implements a K-component Gaussian mixture likelihood. 10.2.0.4 4. Generated Quantities block generated quantities { matrix[N, 2] x_sim; for (n in 1:N) { int z; z = categorical_rng(pi); // sample a component label x_sim[n] = to_row_vector(normal_rng(mu[z], sigma)); } } After fitting, this simulates a new dataset x_sim of the same size: Draw a component label z from Categorical(pi). Draw a 2-vector from Normal(mu[z], σ) in each coordinate. This gives you posterior-predictive (replicated) data from the inferred mixture. 10.2.1 In plain English You are fitting a K-component mixture of 2D, spherical Gaussians to your data. π are the mixing weights, μₖ are the 2D cluster centers, σ is the common cluster “spread.” The sampler will infer the posterior distribution over these parameters, and then generate a new simulated dataset of size N from that fitted mixture. 10.3 More examples Clustering and Latent Dirichlet "],["likelihood-free-inference-via-classification.html", "11 Likelihood-Free Inference via Classification 11.1 Motivation 11.2 Likelihood-Free Inference: The Big Picture 11.3 What is ABC? 11.4 When to Use ABC 11.5 The ABC Rejection Algorithm 11.6 Illustrated Example: Estimating a Mean 11.7 Acceptance Threshold Visualization 11.8 ABC Approximate Posterior 11.9 Extensions of ABC 11.10 References about ABC 11.11 Classification-Based LFI 11.12 Algorithm Sketch 11.13 Motivation 11.14 Likelihood-Free Inference: The Big Picture 11.15 Classification-Based LFI 11.16 Algorithm Sketch 11.17 Example: 1D Gaussian Mixture 11.18 Applications &amp; Case Studies 11.19 Practical Tips 11.20 Advantages &amp; Limitations 11.21 Summary &amp; Outlook 11.22 References", " 11 Likelihood-Free Inference via Classification Gutmann, M. U., Dutta, R., Kaski, S., &amp; Corander, J. (2018). Likelihood-Free Inference via Classification. Statistics and Computing. 11.1 Motivation Many complex simulators → no tractable likelihood Approximate Bayesian Computation (ABC) sidesteps likelihood by simulating and comparing summaries But choosing summaries and distance thresholds is ad hoc Gutmann et al. (2018) propose learning a classifier to distinguish observed vs. simulated and use its outputs for inference 11.2 Likelihood-Free Inference: The Big Picture Generative model \\(p(x\\mid\\theta)\\) is a black-box simulator Observed data \\(x_{\\rm obs}\\) Goal: Approximate \\(p(\\theta\\mid x_{\\rm obs})\\) without ever evaluating \\(p(x\\mid\\theta)\\) 11.3 What is ABC? Approximate Bayesian Computation (ABC) A likelihood‐free inference method Replace an intractable likelihood with simulation + summary statistics Accept parameter draws that produce simulated data “close” to the observed data 11.4 When to Use ABC Complex generative models with intractable or expensive likelihoods Examples: population genetics, epidemiology, systems biology Enables Bayesian inference without ever evaluating \\(p(x\\mid\\theta)\\) 11.5 The ABC Rejection Algorithm # Observed summary statistic s_obs &lt;- summary_statistic(x_obs) # Rejection ABC accepted &lt;- numeric() for (i in seq_len(N)) { theta_i &lt;- sample_prior() x_sim &lt;- simulator(theta_i) s_sim &lt;- summary_statistic(x_sim) if (abs(s_sim - s_obs) &lt; epsilon) { accepted &lt;- c(accepted, theta_i) } } posterior_samples &lt;- accepted 11.6 Illustrated Example: Estimating a Mean set.seed(42) # 1. Observed data x_obs &lt;- rnorm(50, mean = 5, sd = 2) s_obs &lt;- mean(x_obs) # 2. Simulate candidates thetas &lt;- runif(10000, 0, 10) sims &lt;- sapply(thetas, function(theta) mean(rnorm(50, theta, 2))) df &lt;- data.frame(theta = thetas, s_sim = sims) # 3. Histogram of summary statistics ggplot(df, aes(x = s_sim)) + geom_histogram(binwidth = 0.1, fill = &quot;gray80&quot;, color = &quot;white&quot;) + geom_vline(xintercept = s_obs, color = &quot;red&quot;, size = 1) + labs( title = &quot;Simulated Means vs. Observed Mean&quot;, x = &quot;Simulated Mean&quot;, y = &quot;Count&quot; ) 11.7 Acceptance Threshold Visualization epsilon &lt;- 0.1 df$accepted &lt;- abs(df$s_sim - s_obs) &lt; epsilon ggplot(df, aes(x = theta, y = s_sim, color = accepted)) + geom_point(alpha = 0.5) + scale_color_manual(values = c(&quot;FALSE&quot; = &quot;gray70&quot;, &quot;TRUE&quot; = &quot;steelblue&quot;)) + geom_hline(yintercept = s_obs, linetype = &quot;dashed&quot;) + labs( title = &quot;ABC Acceptance Region&quot;, x = expression(theta), y = &quot;Simulated Mean&quot;, color = &quot;Accepted&quot; ) 11.8 ABC Approximate Posterior post &lt;- df$theta[df$accepted] ggplot(data.frame(theta = post), aes(x = theta)) + geom_density(fill = &quot;skyblue&quot;, alpha = 0.6) + labs( title = &quot;ABC Approximate Posterior for θ&quot;, x = expression(theta), y = &quot;Density&quot; ) 11.9 Extensions of ABC ABC‐SMC: Sequential Monte Carlo to adaptively reduce \\(\\varepsilon\\) ABC‐MCMC: MCMC scheme with ABC acceptance step Regression Adjustment: post‐processing to correct bias in posterior 11.10 References about ABC Beaumont, M. A., Zhang, W., &amp; Balding, D. J. (2002). Approximate Bayesian computation in population genetics. Genetics, 162(4), 2025–2035. Marin, J.-M., Pudlo, P., Robert, C. P., &amp; Ryder, R. J. (2012). Approximate Bayesian computational methods. Statistics and Computing, 22(6), 1167–1180. Sisson, S. A., Fan, Y., &amp; Beaumont, M. (2018). Handbook of Approximate Bayesian Computation. CRC Press. 11.11 Classification-Based LFI Simulate \\(\\{\\theta_i,\\,x_i\\}\\) from the prior and simulator Label data: \\(y=1\\) for \\(x_i\\sim p(x\\mid\\theta_i)\\) close to \\(x_{\\rm obs}\\) \\(y=0\\) for simulated from other \\(\\theta\\) Train a discriminator \\(D(x)\\approx P(y=1\\mid x)\\) Use \\(D(x_{\\rm obs})\\) (or its logit) as a surrogate likelihood 11.12 Algorithm Sketch Pseudo-code: for (t in 1:T) { θ_samples &lt;- sample_prior(N) x_sims &lt;- simulator(θ_samples) y_labels &lt;- ifelse(distance(x_sims, x_obs) &lt; ε_t, 1, 0) D_t &lt;- train_classifier(x_sims, y_labels) # Use D_t to weight or propose new θ in SMC/PMC scheme } 11.13 Motivation Many complex simulators → no tractable likelihood Approximate Bayesian Computation (ABC) sidesteps likelihood by simulating and comparing summaries Manual choice of summaries and thresholds can be ad hoc Gutmann et al. (2018) propose using a classifier to distinguish observed vs. simulated, turning that into a discrepancy 11.14 Likelihood-Free Inference: The Big Picture Simulator: black-box generative model \\[p(x \\mid \\theta)\\] Observed data: \\[x_{\\mathrm{obs}}\\] Goal: Approximate posterior \\[p(\\theta \\mid x_{\\mathrm{obs}})\\] without evaluating \\[p(x \\mid \\theta)\\] 11.15 Classification-Based LFI Draw \\((\\theta_i, x_i)_{i=1}^N\\) from the prior and simulator Label each \\(x_i\\): \\(y_i = 1\\) if \\(x_i\\) is “close” to \\(x_{\\mathrm{obs}}\\) \\(y_i = 0\\) otherwise Train classifier \\(D(x)\\approx P(y=1 \\mid x)\\) Use classifier output \\(D(x_{\\mathrm{obs}})\\) or its logit as a surrogate likelihood 11.16 Algorithm Sketch Pseudocode for (t in seq_len(T)) { theta &lt;- sample_prior(N) x_sim &lt;- simulator(theta) y &lt;- as.integer(distance(x_sim, x_obs) &lt; eps[t]) D &lt;- train_classifier(x_sim, y) # Use D to weight or propose new theta in an SMC/PMC loop } 11.17 Example: 1D Gaussian Mixture set.seed(123) n &lt;- 500; p &lt;- 0.3 # Observed: two-component Gaussian mixture obs &lt;- data.frame( x = c(rnorm(n*p, -2, 1), rnorm(n*(1-p), 3, 0.5)), label = &quot;Observed&quot; ) # Simulated under wrong single-Gaussian model sim &lt;- data.frame( x = rnorm(n, 0.5, 1.5), label = &quot;Simulated&quot; ) df &lt;- rbind(obs, sim) # Scatter plot library(ggplot2) ggplot(df, aes(x = x, y = 0, color = label)) + geom_jitter(height = 0.1, alpha = 0.6) + theme_minimal() + labs(title = &quot;Observed vs. Simulated Samples&quot;, x = &quot;Value&quot;, y = &quot;&quot;, color = &quot;&quot;) 11.17.1 Example: Logistic Discriminator \\[ D(x) = \\mathrm{logit}^{-1}\\bigl(\\beta_0 + \\beta_1 x\\bigr) \\] # Fit logistic regression df$y_bin &lt;- ifelse(df$label == &quot;Observed&quot;, 1, 0) fit &lt;- glm(y_bin ~ x, data = df, family = binomial) # ROC curve library(pROC) probs &lt;- predict(fit, type = &quot;response&quot;) roc_obj &lt;- roc(df$y_bin, probs) auc_val &lt;- auc(roc_obj) # Plot ROC roc_df &lt;- data.frame( fpr = rev(roc_obj$specificities), tpr = rev(roc_obj$sensitivities) ) library(ggplot2) ggplot(roc_df, aes(x = fpr, y = tpr)) + geom_line(linewidth = 1) + geom_abline(lty = 2) + theme_minimal() + labs(title = sprintf(&quot;ROC Curve (AUC = %.2f)&quot;, auc_val), x = &quot;False Positive Rate&quot;, y = &quot;True Positive Rate&quot;) 11.18 Applications &amp; Case Studies Population genetics: coalescent simulators for demographic inference Systems biology: stochastic models of biochemical networks Epidemiology: agent-based epidemic simulators 11.19 Practical Tips Classifier choice: Logistic regression for speed Random forests / GBMs for nonlinearity Neural nets for high-dimensional data Features: raw data vs. summary statistics Simulation budget: trade-off between classifier accuracy and compute cos Sequential schemes: embed into SMC-ABC to adapt \\(\\varepsilon\\) 11.20 Advantages &amp; Limitations Pros Cons Automatic, data-driven discrepancy Classifier may fit simulator quirks Continuous distance (AUC, logit) Extra training overhead Integrates into ABC/SMC frameworks Requires careful label calibration 11.21 Summary &amp; Outlook Classification reframes ABC as a supervised learning problem Provides continuous, interpretable discrepancy measures Future directions: Deep architectures for high-dimensional simulators Amortized inference for rapid repeated queries Hybrid likelihood-based &amp; classification approaches 11.22 References Gutmann, M. U., Dutta, R., Kaski, S., &amp; Corander, J. (2018). Likelihood-Free Inference via Classification. Statistics and Computing. Cranmer, K., Brehmer, J., &amp; Louppe, G. (2020). The Frontier of Simulation-Based Inference. PNAS. Papamakarios, G. &amp; Murray, I. (2016). Fast ε-Free Inference of Simulation Models with Bayesian Conditional Density Estimation. NeurIPS. Gutmann, M. U. &amp; Hyvärinen, A. (2012). Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models. AISTATS. "],["all-the-tools-already-available.html", "12 All the tools already available 12.1 A CRAN view of what exists: 12.2 Specific Packages:", " 12 All the tools already available 12.1 A CRAN view of what exists: https://cran.r-project.org/web/views/ExperimentalDesign.html 12.2 Specific Packages: acebayes: Optimal Bayesian Experimental Design using the ACE Algorithm adaptDiag: Bayesian Adaptive Designs for Diagnostic Trials ADCT: Adaptive Design in Clinical Trials adoptr: Adaptive Optimal Two-Stage Designs in R adpss: Design and Analysis of Locally or Globally Efficient Adaptive Designs agricolae: Statistical Procedures for Agricultural Research agricolaeplotr: Visualization of Design of Experiments from the ‘agricolae’ Package AlgDesign: Algorithmic Experimental Design ALTopt: Optimal Experimental Designs for Accelerated Life Testing Aoptbdtvc: A-Optimal Block Designs for Comparing Test Treatments with Controls asd: Simulations for Adaptive Seamless Designs bacistool: Bayesian Classification and Information Sharing (BaCIS) Tool for the Design of Multi-Group Phase II Clinical Trials BayesCR: Bayesian Analysis of Censored Regression Models Under Scale Mixture of Skew Normal Distributions bayesCT: Simulation and Analysis of Adaptive Bayesian Clinical Trials BayesDesign: Bayesian Single-Arm Design with Survival Endpoints BayesPPD: Bayesian Power Prior Design BDEsize: Efficient Determination of Sample Size in Balanced Design of Experiments bdlp: Transparent and Reproducible Artificial Data Generation bioOED: Sensitivity Analysis and Optimum Experiment Design for Microbial Inactivation deepgp: Sequential Design for Deep Gaussian Processes using MCMC DelayedEffect.Design: Sample Size and Power Calculations using the APPLE and SEPPLE Methods demu: Optimal Design Emulators via Point Processes DiceDesign: Designs of Computer Experiments DiceKriging: Kriging Methods for Computer Experiments DiceOptim: Kriging-Based Optimization for Computer Experiments earlyR: Estimation of Transmissibility in the Early Stages of a Disease Outbreak easypower: Sample Size Estimation for Experimental Designs experDesign: Design Experiments for Batches faux: Simulation for Factorial Designs FielDHub: A Shiny App for Design of Experiments in Life Sciences FieldSim: Random Fields (and Bridges) Simulations geospt: Geostatistical Analysis and Design of Optimal Spatial Sampling Networks gscounts: Group Sequential Designs with Negative Binomial Outcomes hiPOD: hierarchical Pooled Optimal Design MCPModPack: Simulation-Based Design and Analysis of Dose-Finding Trials scDesign: Simulation-based scRNA-seq experimental design phyclust: (seqgen) Simulate the evolution of nucleotide or amino acid sequences along a phylogeny splatter: Simulation of single-cell RNA sequencing count data MSstatsSampleSize: Simulation tool for optimal design of high-dimensional MS-based proteomics experiment PROPER: Simulation based methods for evaluating the statistical power in differential expression analysis from RNA-seq data MOSim: Simulates multi-omic experiments that mimic regulatory mechanisms within the cell mbtransfer: Simulation of hypothetical community trajectories under user-specified perturbations. miniLNM :For fitting and using logistic-normal multinomial models. It wraps a simple ‘Stan’ script. multimedia : Multimodal mediation analysis of microbiome data scDesign2 "],["multimodal-data-examples-of-approaches.html", "13 Multimodal data examples of approaches 13.1 Special data structures", " 13 Multimodal data examples of approaches There are some good examples for the microbiome that care instructuve for other domains. Multimedia website install.packages(&quot;multimedia&quot;) ####or you can use the new version: devtools::install_github(&quot;krisrs1128/multimedia&quot;) BiocManager::install(&quot;phyloseq&quot;) 13.1 Special data structures Summarized Experiment About MultiAssayExperiments About phyloseq Lecture on Microbiome Data Computational Challenges of Multidomain Data Complex dependent Factors for Multiway Data "],["other-examples-of-applications.html", "14 Other examples of applications", " 14 Other examples of applications Semi synthetic data enrichment Why we shouldn’t be using compositional methods for Microbiome or Transcriptome data "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
