[["workshop-on-using-generative-models.html", "Generative Modeling Workshop 1 Workshop on using generative models 1.1 Our learning goals: 1.2 Prerequisites:", " Generative Modeling Workshop Susan Holmes May 8, 2025 1 Workshop on using generative models Nova University, Lisbon, 7 and 8 May, 2025 1.1 Our learning goals: Learn how to use simulation tools to solve and understand statistical models. Go over examples of experimental design using generative models. Useful ideas when using experimental simulations to communicate with collaborators: design, non-identifiability, decision making under uncertainty. Use different R packages to simulate data according to a hierarchical model with multiple levels of random variation. Overview of some modern approaches to simulation: Monte Carlo, stan, Bayesian approaches. 1.2 Prerequisites: Some statistical know-how and R and RStudio installed on your laptops. Some useful buzzwords and acronyms to look up in case: GOF: Goodness of Fit MCMC: Markov chain Monte Carlo ABC: Approximate Bayesian Computation Random generation: From uniform and non-uniform distributions. Mixtures: Finite combinations of known distributions Hierarchical Models: Infinite mixtures GANs: Generative Adversarial network Identifiability GMM: Gaussian Mixture Models (beware GMM : Generalized Method of Moments) GLM : Generalized Linear Models GLMM: General Linear Mixture Models HMC: Hamiltonian Monte Carlo "],["beyond-black-box-simulation.html", "2 Beyond Black Box Simulation 2.1 New Lingua Franca of Science", " 2 Beyond Black Box Simulation Beyond Black Box Simulation Lisbon Nova [7 | May | 2025] Paper: https://go.wisc.edu/833zs8 Code: https://go.wisc.edu/7222i9 Susan Holmes Stanford University joint work with, Kris Sankaran, UW-Madison 2.1 New Lingua Franca of Science Simulators have emerged as a general problem-solving device across various domains, many of which now have rich, open-source libraries. Where is the interface with statistics? Experimental design, model building, and decision-making. The E3SM is used for long-term climate projections. 2.1.1 New Lingua Franca of Science Simulators have emerged as a general problem-solving device across various domains, many of which now have rich, open-source libraries. Where is the interface with statistics? Experimental design, model building, and decision-making. Splatter generates synthetic single-cell genomics data. 2.1.2 Grammar of Generative Models Transparent simulators can be built by interactively composing simple modules. Probabilistic programming has simplified the process. Regression b: Hierarchy c: Latent Structure d: Temporal Variation 2.1.3 Discrepancy and Iterability By learning a discriminator to contrast real vs. simulated data, we can systematically improve the assumed generative mechanism. 2.1.4 Experimental Design Renaissance Let’s consider a microbiomics case study: To block or not to block? Blocking removes person-level effects… …but increases participant burden. 2.1.5 Simulation to the Rescue How can we navigate trade-offs like this? Simulate! Simulators provide data for more precise decision-making. 2.1.6 Covasim Following the outbreak of COVID-19, the research community came together to build simulators that could inform pandemic response. * E.g., “What would happen if we held classes remotely for two weeks?” 2.1.7 Covasim Covasim is an example of an agent-based model. Starting from local interaction rules, it lets us draw global inferences. Statistical emulators mimic the relationship between input hyperparameters and output data, substantially reducing the computational burden. Inference and imagination: Statistical calibration grounds us in reality while generative tinkering encourages us to imagine. 2.1.8 What are we going to study ? Many examples, like showing non-identifiability, the evolution of mimicry in butterflies, longitudinal study design, the duality between agents and particles, … * Paper Link: https://go.wisc.edu/833zs8 * Code (R + Python + NetLogo): https://go.wisc.edu/7222i9 A github repository of the material is available here: https://spholmes.github.io/Generative_Modeling/ "],["slides.html", "3 Slides", " 3 Slides NotBlackBoxSimulation Experimental Design "],["labs-and-challenges.html", "4 Labs and Challenges :", " 4 Labs and Challenges : Find code for an example of a hierarchical generative model in the book: Modern Statistics for Modern Biology Install the package generative from the github repository spholme/generativeusing the packagedevtools` with devtools::install_github(&quot;spholmes/generative&quot;) Try out examples from the repository of the paper at: https://go.wisc.edu/833zs8 Code: https://go.wisc.edu/7222i9 Install Stan : you can ask a LLM to help for instance chatGPT provided: https://chatgpt.com/share/681bc493-1a64-8005-8d07-cc77cff145d7 Do the examples in the modeling and simulation folders of the generative-review github repo. Download the package mbtransfer and inspect the simulation component of the package (there is a CRAN and a github version). "],["non-identifiability-in-mixture-models-an-example.html", "5 Non-Identifiability in Mixture Models: An Example 5.1 What is Non-Identifiability? 5.2 The Binomial Mixture Model 5.3 Generating Mixture Distributions 5.4 Visualizing the Probability Mass Functions 5.5 Comparing Empirical Distributions 5.6 Statistical Comparison of Distributions 5.7 Testing the Empirical Similarity 5.8 Expected vs Observed Distributions 5.9 Summary", " 5 Non-Identifiability in Mixture Models: An Example 5.1 What is Non-Identifiability? Non-identifiability occurs when multiple different parameter values lead to identical (or nearly identical) probability distributions. In such cases, it becomes challenging or impossible to determine the true parameters from observed data, as different parameter combinations can explain the same observations equally well. This tutorial demonstrates this concept using a mixture of two binomial distributions. We’ll show how different combinations of mixture proportions and probability parameters can generate virtually indistinguishable data. 5.2 The Binomial Mixture Model Consider a mixture of two binomial distributions with the following parameters: - Mixture proportion: \\(\\alpha\\) and \\((1-\\alpha)\\) - Success probabilities: \\(p_1\\) and \\(p_2\\) - Number of trials: \\(n\\) The probability mass function is: \\[P(X=k) = \\alpha \\binom{n}{k} p_1^k (1-p_1)^{n-k} + (1-\\alpha) \\binom{n}{k} p_2^k (1-p_2)^{n-k}\\] In a mixture of two binomial distributions, we have: - Two binomial components with success probabilities p₁ and p₂ - A mixing proportion α (where 0 &lt; α &lt; 1) - The resulting probability mass function is: α·Binom(n, p₁) + (1-α)·Binom(n, p₂) The non-identifiability problem occurs because different combinations of (α, p₁, p₂) can produce very similar observed distributions. 5.2.1 Defining Parameter Sets Let’s define two different parameter sets that we suspect might produce similar distributions: # Number of trials for each binomial n &lt;- 20 # Parameter set 1 alpha1 &lt;- 0.7 p1_1 &lt;- 0.3 p2_1 &lt;- 0.8 # Parameter set 2 alpha2 &lt;- 0.4 p1_2 &lt;- 0.2 p2_2 &lt;- 0.6 # Parameter set 3 (even more different) alpha3 &lt;- 0.6 p1_3 &lt;- 0.6 p2_3 &lt;- 0.2 5.3 Generating Mixture Distributions Now let’s generate the probability mass functions for each parameter set: # Function to compute the PMF for a mixture of two binomials mixture_binomial_pmf &lt;- function(n, alpha, p1, p2) { x &lt;- 0:n pmf &lt;- alpha * dbinom(x, n, p1) + (1-alpha) * dbinom(x, n, p2) return(data.frame(x = x, probability = pmf)) } # Generate PMFs for each parameter set pmf1 &lt;- mixture_binomial_pmf(n, alpha1, p1_1, p2_1) pmf2 &lt;- mixture_binomial_pmf(n, alpha2, p1_2, p2_2) pmf3 &lt;- mixture_binomial_pmf(n, alpha3, p1_3, p2_3) 5.4 Visualizing the Probability Mass Functions Let’s plot the PMFs side by side to see how similar they are: # Combine the data for plotting pmf1$set &lt;- &quot;Set 1: α=0.7, p₁=0.3, p₂=0.8&quot; pmf2$set &lt;- &quot;Set 2: α=0.4, p₁=0.2, p₂=0.6&quot; pmf3$set &lt;- &quot;Set 3: α=0.6, p₁=0.6, p₂=0.2&quot; combined_pmf &lt;- rbind(pmf1, pmf2, pmf3) # Plotting all three distributions ggplot(combined_pmf, aes(x = x, y = probability, fill = set)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;, alpha = 0.7) + labs(title = &quot;Probability Mass Functions for Different Parameter Sets&quot;, x = &quot;Number of Successes&quot;, y = &quot;Probability&quot;, fill = &quot;Parameter Set&quot;) + theme_minimal() + scale_fill_brewer(palette = &quot;Set1&quot;) 5.4.1 Generating Samples from Each Distribution Let’s generate samples from each parameter set to see how the empirical distributions compare: # Function to generate samples from a mixture of binomials generate_mixture_samples &lt;- function(n_samples, n_trials, alpha, p1, p2) { # For each sample, decide which binomial to use component &lt;- rbinom(n_samples, 1, alpha) # Generate samples using the appropriate binomial samples &lt;- numeric(n_samples) samples[component == 1] &lt;- rbinom(sum(component), n_trials, p1) samples[component == 0] &lt;- rbinom(sum(1-component), n_trials, p2) return(samples) } # Sample size n_samples &lt;- 1000 # Generate samples samples1 &lt;- generate_mixture_samples(n_samples, n, alpha1, p1_1, p2_1) samples2 &lt;- generate_mixture_samples(n_samples, n, alpha2, p1_2, p2_2) samples3 &lt;- generate_mixture_samples(n_samples, n, alpha3, p1_3, p2_3) 5.5 Comparing Empirical Distributions # Prepare data for plotting samples_df &lt;- data.frame( value = c(samples1, samples2, samples3), set = c(rep(&quot;Set 1: α=0.7, p₁=0.3, p₂=0.8&quot;, n_samples), rep(&quot;Set 2: α=0.4, p₁=0.2, p₂=0.6&quot;, n_samples), rep(&quot;Set 3: α=0.6, p₁=0.6, p₂=0.2&quot;, n_samples)) ) # Create histograms for each sample ggplot(samples_df, aes(x = value, fill = set)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5, bins = n+1) + facet_wrap(~set, ncol = 1) + labs(title = &quot;Empirical Distributions from Different Parameter Sets&quot;, x = &quot;Number of Successes&quot;, y = &quot;Frequency&quot;) + theme_minimal() + scale_fill_brewer(palette = &quot;Set1&quot;) 5.6 Statistical Comparison of Distributions Let’s compute some summary statistics to compare these distributions more formally: # Function to compute summary statistics compute_summary &lt;- function(samples, label) { data.frame( Set = label, Mean = mean(samples), SD = sd(samples), Q25 = quantile(samples, 0.25), Median = median(samples), Q75 = quantile(samples, 0.75) ) } # Compute summary statistics stats1 &lt;- compute_summary(samples1, &quot;Set 1: α=0.7, p₁=0.3, p₂=0.8&quot;) stats2 &lt;- compute_summary(samples2, &quot;Set 2: α=0.4, p₁=0.2, p₂=0.6&quot;) stats3 &lt;- compute_summary(samples3, &quot;Set 3: α=0.6, p₁=0.6, p₂=0.2&quot;) # Combine and display summary_stats &lt;- rbind(stats1, stats2, stats3) knitr::kable(summary_stats, digits = 3) Set Mean SD Q25 Median Q75 25% Set 1: α=0.7, p₁=0.3, p₂=0.8 8.824 4.967 5 7 14 25%1 Set 2: α=0.4, p₁=0.2, p₂=0.6 8.699 4.359 5 10 12 25%2 Set 3: α=0.6, p₁=0.6, p₂=0.2 8.607 4.478 4 10 12 5.7 Testing the Empirical Similarity Let’s perform a Kolmogorov-Smirnov test to check if the empirical distributions are statistically distinguishable: # Perform KS tests between each pair of distributions ks_test_1_vs_2 &lt;- ks.test(samples1, samples2) ks_test_1_vs_3 &lt;- ks.test(samples1, samples3) ks_test_2_vs_3 &lt;- ks.test(samples2, samples3) # Display results ks_results &lt;- data.frame( Comparison = c(&quot;Set 1 vs Set 2&quot;, &quot;Set 1 vs Set 3&quot;, &quot;Set 2 vs Set 3&quot;), D_statistic = c(ks_test_1_vs_2$statistic, ks_test_1_vs_3$statistic, ks_test_2_vs_3$statistic), p_value = c(ks_test_1_vs_2$p.value, ks_test_1_vs_3$p.value, ks_test_2_vs_3$p.value) ) knitr::kable(ks_results, digits = 4) Comparison D_statistic p_value Set 1 vs Set 2 0.186 0.0000 Set 1 vs Set 3 0.186 0.0000 Set 2 vs Set 3 0.034 0.6099 5.8 Expected vs Observed Distributions Let’s visualize how well the theoretical PMFs match the empirical distributions: # Create a function to plot expected vs observed for a given parameter set plot_expected_vs_observed &lt;- function(samples, pmf, title) { observed &lt;- as.data.frame(table(factor(samples, levels = 0:n))) colnames(observed) &lt;- c(&quot;x&quot;, &quot;frequency&quot;) observed$x &lt;- as.numeric(as.character(observed$x)) observed$proportion &lt;- observed$frequency / sum(observed$frequency) # Combine for plotting comparison &lt;- merge(observed, pmf, by = &quot;x&quot;) comparison$type &lt;- &quot;Observed&quot; comparison$expected &lt;- comparison$probability comparison$observed &lt;- comparison$proportion # Create a long format for ggplot plot_data &lt;- data.frame( x = rep(comparison$x, 2), proportion = c(comparison$observed, comparison$expected), type = c(rep(&quot;Observed&quot;, nrow(comparison)), rep(&quot;Expected&quot;, nrow(comparison))) ) # Create the plot ggplot(plot_data, aes(x = x, y = proportion, fill = type)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;, alpha = 0.7) + labs(title = title, x = &quot;Number of Successes&quot;, y = &quot;Probability/Proportion&quot;, fill = &quot;&quot;) + theme_minimal() + scale_fill_manual(values = c(&quot;Observed&quot; = &quot;darkblue&quot;, &quot;Expected&quot; = &quot;orange&quot;)) } # Create plots for each parameter set p1 &lt;- plot_expected_vs_observed(samples1, pmf1, &quot;Set 1: α=0.7, p₁=0.3, p₂=0.8&quot;) p2 &lt;- plot_expected_vs_observed(samples2, pmf2, &quot;Set 2: α=0.4, p₁=0.2, p₂=0.6&quot;) p3 &lt;- plot_expected_vs_observed(samples3, pmf3, &quot;Set 3: α=0.6, p₁=0.6, p₂=0.2&quot;) # Arrange the plots in a grid grid.arrange(p1, p2, p3, ncol = 1) 5.9 Summary This example demonstrates the non-identifiability issue in mixtures of binomial distributions. We’ve shown how different combinations of parameters (α, p₁, p₂) can produce very similar observed data distributions. This makes it challenging to recover the true parameters from observed data alone without additional constraints or prior information. The key insights from this demonstration: Different parameter sets can produce nearly indistinguishable probability distributions The empirical distributions from simulated data are also very similar Standard statistical tests struggle to differentiate between samples generated from these different parameter combinations This non-identifiability issue is a fundamental challenge in mixture modeling and highlights the importance of incorporating domain knowledge or imposing constraints when working with such models. Can you think of other simple examples of non-identifiability? Clustering. Very correlated variables. "],["intervention.html", "6 Intervention 6.1 Start defining the intervention in a time series", " 6 Intervention 6.1 Start defining the intervention in a time series library(tidyverse) library(splines) library(generative) set.seed(123) times &lt;- seq(-10, 10, length.out = 200) pf &lt;- peak_fun(times, 1, 10, 5) plot(times, pf(times), type = &quot;l&quot;) u &lt;- runif(20, -10, 10) points(u, gaussian_noise(pf, u)) u &lt;- seq(-10, 10, length.out = 10) intervals &lt;- cbind(head(u, -1), tail(u, -1)) weights &lt;- 5:1 weights &lt;- weights / sum(weights) locs &lt;- weighted_sampling(1e4, intervals, weights) hist(locs) We now have a way of simulating parameterized mean functions and observations with noise around them We also have ways of sampling locations to observe the function, based on a weighted combination of uniforms We need a way of estimating these mean functions and evaluating the resulting fit. We also need to evaluate many combinations of weightings, either using random search or bayesian optimization First, we attempt to estimate these mean functions using simple piecewise linear splines w &lt;- c(1, 1, 1, 1, 3, 4, 3, 1, 1) w &lt;- w / sum(w) u &lt;- weighted_sampling(40, intervals, w) y &lt;- gaussian_noise(pf, u) fit &lt;- lm(y ~ bs(u, df = 6, degree = 1, Boundary.knots = c(-10, 10))) plot(times, pf(times), type = &quot;l&quot;) points(u, y, col = &quot;red&quot;) points(u, predict(fit), col = &quot;blue&quot;) f_hat &lt;- predict(fit, newdata = data.frame(u = times)) points(times, f_hat, col = &quot;blue&quot;) B &lt;- 1e3 mse &lt;- matrix(nrow = B, ncol = 2) for (b in seq_len(B)) { # random u&#39;s u &lt;- runif(50, -10, 10) mse[b, 1] &lt;- spline_mse(u, times, pf) u &lt;- weighted_sampling(50, intervals, w) mse[b, 2] &lt;- spline_mse(u, times, pf) } colnames(mse) &lt;- c(&quot;uniform&quot;, &quot;weighted&quot;) mse &lt;- as_tibble(mse) %&gt;% mutate(id = 1:n()) %&gt;% pivot_longer(-id, names_to = &quot;sampling&quot;, values_to = &quot;mse&quot;) ggplot(mse) + geom_histogram( aes(mse, fill = sampling), alpha = 0.8, position = &quot;identity&quot;, bins = 50 ) + scale_y_continuous(expand = c(0, 0, 0.1, 0)) + scale_x_continuous(expand = c(0, 0)) "],["intervention_optimize.html", "7 Intervention_Optimize 7.1 We are going to choose the best points", " 7 Intervention_Optimize 7.1 We are going to choose the best points (this can be slow) library(tidyverse) library(laGP) library(purrr) library(generative) library(splines) review_theme() set.seed(123) ### This is very slow n_init &lt;- 100 n_interval &lt;- 7 intervals &lt;- make_intervals(n_interval) times &lt;- seq(-10, 10, length.out = 200) pf &lt;- peak_fun(times, 1, 10, 5) # initial sample of weights betas &lt;- rerun(n_init, rnorm(n_interval, 0, 1)) losses &lt;- map_dbl(betas, ~ evaluate_weights(., intervals, times, pf, N = 25)) We compute the main Bayesian optimization loop below. n_test &lt;- 500 n_add &lt;- 10 n_iters &lt;- 50 for (i in seq_len(n_iters)) { # fit a GP to the betas / losses already computed beta_norm &lt;- scale_list(betas) beta_test &lt;- rerun(n_test, runif(n_interval, -4, 4)) # find most promising new test points y_hat &lt;- aGP(beta_norm, losses, scale_list(beta_test), verb = 0) sort_ix &lt;- order(y_hat$mean - y_hat$var) # evaluate loss on new test points new_betas &lt;- beta_test[sort_ix][1:n_add] betas &lt;- c(betas, new_betas) losses &lt;- c( losses, map_dbl(new_betas, ~ evaluate_weights(., intervals, times, pf, N = 25)) ) } Here are some of the weightings that we should use for longitudinal sampling. They generally favor more samples in the region where the peak is located. w &lt;- bind_rows(map(betas, ~ data.frame(exp(t(.))))) w &lt;- w / rowSums(w) w$loss &lt;- losses library(embed) library(tidymodels) w_reduce &lt;- recipe(~ ., data = w) %&gt;% update_role(loss, new_role = &quot;id&quot;) umap_rec &lt;- step_umap(w_reduce, all_predictors(), neighbors = 30, min_dist = 0.05) scores &lt;- prep(umap_rec) %&gt;% juice() %&gt;% bind_cols(w %&gt;% select(-loss)) ggplot(scores) + geom_point(aes(UMAP1, UMAP2, col = log(loss), size = X4)) + scale_color_viridis_c(direction = -1, limits = c(0, 8), option = &quot;inferno&quot;) w_df &lt;- w %&gt;% mutate(id = row_number()) %&gt;% arrange(-loss) %&gt;% pivot_longer(starts_with(&quot;X&quot;), names_to = &quot;component&quot;) intervals_df &lt;- as.data.frame(intervals) intervals_df$component &lt;- paste0(&quot;X&quot;, 1:nrow(intervals)) w_df &lt;- w_df %&gt;% left_join(intervals_df) ggplot(w_df %&gt;% filter(log(loss) &lt; 0.08)) + geom_rect(aes(xmin = V1, xmax = V2, ymin = 0, ymax = value), fill = &quot;#262526&quot;) + scale_x_continuous(expand = c(0, 0), breaks = c(-5, 0, 5)) + scale_y_continuous(expand = c(0, 0, 0.1, 0)) + facet_grid(. ~ round(loss, 4)) ggsave(&quot;~/Downloads/best_weights.png&quot;) u &lt;- map(1:8, ~ runif(25, -10, 10)) examples &lt;- map_dfr(u, ~ data.frame( u = ., y = gaussian_noise(pf, .) ), .id = &quot;run&quot; ) %&gt;% mutate(run = as.factor(run)) preds &lt;- examples %&gt;% split(.$run) %&gt;% map_dfr(~ { fit &lt;- lm(y ~ bs(u, df = 6, degree = 1, Boundary.knots = c(-10, 10)), data = .) data.frame( times = times, mu = pf(times), f_hat = predict(fit, newdata = data.frame(u = times)) ) }, .id = &quot;run&quot; ) %&gt;% group_by(run) %&gt;% mutate( run = as.factor(run), loss = (f_hat - mu) ^ 2 ) truth &lt;- data.frame(times = times, mu = pf(times)) run_order &lt;- preds %&gt;% group_by(run) %&gt;% summarise(mse = mean(loss)) %&gt;% arrange(mse) %&gt;% pull(run) preds &lt;- preds %&gt;% mutate(run = factor(run, levels = run_order)) examples &lt;- examples %&gt;% mutate(run = factor(run, levels = run_order)) ggplot(preds) + geom_line(aes(times, mu), col = &quot;#F27507&quot;, linewidth = 2) + geom_line(aes(times, f_hat), col = &quot;#087F8C&quot;, linewidth = 1.5) + geom_point(data = examples, aes(u, y), col = &quot;#221F26&quot;) + labs(y = &quot;y&quot;, x = &quot;time&quot;) + facet_wrap(~ run, ncol = 4) ggsave(&quot;~/Downloads/peak_samples.png&quot;, dpi = 800) "],["heteroskedastic-mixtures-goodness-of-fit-iterations.html", "8 Heteroskedastic Mixtures : goodness of fit iterations", " 8 Heteroskedastic Mixtures : goodness of fit iterations library(tidyverse) library(caret) library(scico) library(generative) review_theme() set.seed(090322) Simulate data from a heteroskedastic mixture of t’s Fit a standard GMM (stan) Fit a discriminator (caret) Visualize discriminator probabilities Fit updated heteroskedastic model Repeat discrimination Fit true model Repeat discrimination disc_data &lt;- list() p &lt;- list() x &lt;- t_mixture(500) ggplot(x) + geom_point(aes(V1, V2)) + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) + coord_fixed() ggsave(&quot;~/Downloads/true_mixture.png&quot;, dpi = 400, width = 5) fit &lt;- fit_gmm(x, K = 4) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.000799 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 7.99 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2745.118 1.000 1.000 ## 200 -2427.204 0.565 1.000 ## 300 -2427.541 0.377 0.131 ## 400 -2425.820 0.283 0.131 ## 500 -2425.803 0.226 0.001 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 1.8 seconds. x_sim &lt;- mixture_predictive(fit) ggplot(x_sim) + geom_point(aes(`1`, `2`), size = 0.5, alpha = 0.8) + facet_wrap(~ .iteration) + coord_fixed() disc_data[[&quot;gmm&quot;]] &lt;- bind_dicriminative(x, x_sim) gbmGrid &lt;- expand.grid( interaction.depth = 5, n.trees = 100, shrinkage = 0.1, n.minobsinnode = 2 ) trControl &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 10 ) fit &lt;- train( x = disc_data[[&quot;gmm&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;gmm&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.543 0.086 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was ## held constant at a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning ## parameter &#39;n.minobsinnode&#39; was held constant at a value of 2 p[[&quot;gmm&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;gmm&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;gmm&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) fit &lt;- fit_gmm_cov(x, K = 5) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.002167 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 21.67 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2451.708 1.000 1.000 ## 200 -2445.865 0.501 1.000 ## 300 -2445.244 0.334 0.002 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 3.7 seconds. x_sim &lt;- mixture_predictive(fit) disc_data[[&quot;gmm_cov&quot;]] &lt;- bind_dicriminative(x, x_sim) fit &lt;- train( x = disc_data[[&quot;gmm_cov&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;gmm_cov&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.5253 0.0506 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was ## held constant at a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning ## parameter &#39;n.minobsinnode&#39; was held constant at a value of 2 p[[&quot;gmm_cov&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;gmm_cov&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;gmm_cov&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) fit &lt;- fit_tmm(x, K = 5) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## Gradient evaluation took 0.001927 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 19.27 seconds. ## Adjust your expectations accordingly! ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2655.391 1.000 1.000 ## 200 -2649.922 0.501 1.000 ## 300 -2637.090 0.336 0.005 MEDIAN ELBO CONVERGED ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## Finished in 3.4 seconds. x_sim &lt;- mixture_predictive(fit) ggplot(x_sim) + geom_point(aes(`1`, `2`), size = 0.5, alpha = 0.8) + facet_wrap(~ .iteration) + coord_fixed() disc_data[[&quot;tmm&quot;]] &lt;- bind_dicriminative(x, x_sim, iter = 4) fit &lt;- train( x = disc_data[[&quot;tmm&quot;]] %&gt;% select(-y) %&gt;% as.matrix(), y = disc_data[[&quot;tmm&quot;]]$y, method = &quot;gbm&quot;, tuneGrid = gbmGrid, trControl = trControl, verbose = FALSE ) fit ## Stochastic Gradient Boosting ## ## 1000 samples ## 2 predictor ## 2 classes: &#39;simulated&#39;, &#39;true&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... ## Resampling results: ## ## Accuracy Kappa ## 0.5844 0.1688 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning parameter &#39;interaction.depth&#39; was ## held constant at a value of 5 ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## Tuning ## parameter &#39;n.minobsinnode&#39; was held constant at a value of 2 p[[&quot;tmm&quot;]] &lt;- data.frame(prob = predict(fit, disc_data[[&quot;tmm&quot;]], type = &quot;prob&quot;)) %&gt;% bind_cols(disc_data[[&quot;tmm&quot;]]) %&gt;% mutate(y_label = fct_recode(y, &quot;Posterior Samples&quot; = &quot;simulated&quot;, &quot;True Data&quot; = &quot;true&quot;)) p_df &lt;- bind_rows(p, .id = &quot;model&quot;) %&gt;% mutate( model = fct_recode(model, &quot;Gaussian (Shared Covariance)&quot; = &quot;gmm&quot;, &quot;Gaussian (Individual Covariance)&quot; = &quot;gmm_cov&quot;, &quot;Student&#39;s t (Individual Covariance)&quot; = &quot;tmm&quot;), y_label = factor(y_label, levels = c(&quot;True Data&quot;, &quot;Posterior Samples&quot;)) ) ggplot(p_df, aes(V1, V2, col = prob.true)) + geom_point(size = 1.2) + scale_color_scico(palette = &quot;berlin&quot;) + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;, col = &quot;Predicted Probability&quot;) + xlim(-11, 11) + coord_fixed() + facet_grid(y_label ~ model) ggsave(&quot;~/Downloads/modeling_iteration.png&quot;, dpi = 500, width = 1.1 * 7, height = 1.1 * 4.5) for (m in unique(p_df$model)) { p_df %&gt;% filter(y_label == &quot;Posterior Samples&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2)) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/{m}.png&quot;), width = 5.5, height = 4.5) p_df %&gt;% filter(y_label == &quot;Posterior Samples&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2, col = prob.true)) + scale_color_scico(palette = &quot;berlin&quot;) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/{m}-prob.png&quot;), width = 5.5, height = 4.5) p_df %&gt;% filter(y_label == &quot;True Data&quot;, model == m) %&gt;% ggplot() + geom_point(aes(V1, V2, col = prob.true)) + scale_color_scico(palette = &quot;berlin&quot;) + xlim(-11, 11) + coord_fixed() + labs(x = &quot;dimension 1&quot;, y = &quot;dimension 2&quot;) ggsave(glue::glue(&quot;~/Downloads/true-{m}-prob.png&quot;), width = 5.5, height = 4.5) } "],["stan-explained.html", "9 Stan Explained 9.1 Installing stan 9.2 gmm.stan file 9.3 More examples", " 9 Stan Explained 9.1 Installing stan Our suggestion is to use the new cmdstanr 9.2 gmm.stan file Be very careful with the indenting and format data { int&lt;lower=0&gt; K; int&lt;lower=0&gt; N; matrix[N, 2] x; } parameters { simplex[K] pi; matrix[K, 2] mu; real&lt;lower=0&gt; sigma; } model { vector[K] log_prob; for (n in 1:N) { for (k in 1:K) { log_prob[k] = log(pi[k]) + normal_lpdf(x[n] | mu[k], sigma); } target += log_sum_exp(log_prob); } } generated quantities { matrix[N, 2] x_sim; for (n in 1:N) { int z; z = categorical_rng(pi); x_sim[n] = to_row_vector(normal_rng(mu[z], sigma)); } } The Stan file defines a mixture of K Gaussians, with shared sigma, mixture weights pi, and component means mu[k]. The data block specifies K, N, and a 2D matrix x (of size N by 2). In the parameters block, pi is a simplex of size K, mu contains K by 2 means, and sigma is positive. The model computes log_prob[k] for each data point, but yes, since normal_lpdf expects scalar inputs, it sums across dimensions when x[n] and mu[k] are vectors. Thought for a few seconds Here’s what this Stan program does, block by block: 9.2.0.1 1. Data block data { int&lt;lower=0&gt; K; // number of mixture components int&lt;lower=0&gt; N; // number of observations matrix[N, 2] x; // N data points in ℝ² } K: how many Gaussian clusters you want. N: how many 2-dimensional observations. x: your observed data, an N×2 matrix. 9.2.0.2 2. Parameters block parameters { simplex[K] pi; // mixing proportions over K components matrix[K, 2] mu; // 2D mean vector for each component real&lt;lower=0&gt; sigma; // shared (isotropic) standard deviation } pi is a length-K simplex (nonnegative, sums to 1) giving the prior weight of each component. mu[k] is the 2-vector mean of component k. sigma is a single positive scalar: all components share the same spherical (isotropic) standard deviation. 9.2.0.3 3. Model block model { vector[K] log_prob; for (n in 1:N) { for (k in 1:K) { log_prob[k] = log(pi[k]) // log prior weight of component k + normal_lpdf(x[n] | mu[k], sigma); // this expands to the sum of two independent // Normal(μ[k][d], σ) log-densities for d=1,2 } target += log_sum_exp(log_prob); // “log mixture density” trick } } For each data point x[n] and each component k, it computes \\[ \\log\\bigl(\\pi_k\\bigr) + \\sum_{d=1}^2 \\log\\bigl[ \\mathrm{Normal}\\bigl(x_{n,d}\\mid \\mu_{k,d},\\sigma\\bigr) \\bigr]. \\] It then adds \\(\\log\\bigl(\\sum_k \\exp(\\text{that})\\bigr)\\) to the joint log-posterior, which implements a K-component Gaussian mixture likelihood. 9.2.0.4 4. Generated Quantities block generated quantities { matrix[N, 2] x_sim; for (n in 1:N) { int z; z = categorical_rng(pi); // sample a component label x_sim[n] = to_row_vector(normal_rng(mu[z], sigma)); } } After fitting, this simulates a new dataset x_sim of the same size: Draw a component label z from Categorical(pi). Draw a 2-vector from Normal(mu[z], σ) in each coordinate. This gives you posterior-predictive (replicated) data from the inferred mixture. 9.2.1 In plain English You are fitting a K-component mixture of 2D, spherical Gaussians to your data. π are the mixing weights, μₖ are the 2D cluster centers, σ is the common cluster “spread.” The sampler will infer the posterior distribution over these parameters, and then generate a new simulated dataset of size N from that fitted mixture. 9.3 More examples Clustering and Latent Dirichlet "],["likelihood-free-inference-via-classification.html", "10 Likelihood-Free Inference via Classification 10.1 Motivation 10.2 Likelihood-Free Inference: The Big Picture 10.3 What is ABC? 10.4 When to Use ABC 10.5 The ABC Rejection Algorithm 10.6 Illustrated Example: Estimating a Mean 10.7 Acceptance Threshold Visualization 10.8 ABC Approximate Posterior 10.9 Extensions of ABC 10.10 References about ABC 10.11 Classification-Based LFI 10.12 Algorithm Sketch 10.13 Motivation 10.14 Likelihood-Free Inference: The Big Picture 10.15 Classification-Based LFI 10.16 Algorithm Sketch 10.17 Example: 1D Gaussian Mixture 10.18 Applications &amp; Case Studies 10.19 Practical Tips 10.20 Advantages &amp; Limitations 10.21 Summary &amp; Outlook 10.22 References", " 10 Likelihood-Free Inference via Classification Gutmann, M. U., Dutta, R., Kaski, S., &amp; Corander, J. (2018). Likelihood-Free Inference via Classification. Statistics and Computing. 10.1 Motivation Many complex simulators → no tractable likelihood Approximate Bayesian Computation (ABC) sidesteps likelihood by simulating and comparing summaries But choosing summaries and distance thresholds is ad hoc Gutmann et al. (2018) propose learning a classifier to distinguish observed vs. simulated and use its outputs for inference 10.2 Likelihood-Free Inference: The Big Picture Generative model \\(p(x\\mid\\theta)\\) is a black-box simulator Observed data \\(x_{\\rm obs}\\) Goal: Approximate \\(p(\\theta\\mid x_{\\rm obs})\\) without ever evaluating \\(p(x\\mid\\theta)\\) 10.3 What is ABC? Approximate Bayesian Computation (ABC) A likelihood‐free inference method Replace an intractable likelihood with simulation + summary statistics Accept parameter draws that produce simulated data “close” to the observed data 10.4 When to Use ABC Complex generative models with intractable or expensive likelihoods Examples: population genetics, epidemiology, systems biology Enables Bayesian inference without ever evaluating \\(p(x\\mid\\theta)\\) 10.5 The ABC Rejection Algorithm # Observed summary statistic s_obs &lt;- summary_statistic(x_obs) # Rejection ABC accepted &lt;- numeric() for (i in seq_len(N)) { theta_i &lt;- sample_prior() x_sim &lt;- simulator(theta_i) s_sim &lt;- summary_statistic(x_sim) if (abs(s_sim - s_obs) &lt; epsilon) { accepted &lt;- c(accepted, theta_i) } } posterior_samples &lt;- accepted 10.6 Illustrated Example: Estimating a Mean set.seed(42) # 1. Observed data x_obs &lt;- rnorm(50, mean = 5, sd = 2) s_obs &lt;- mean(x_obs) # 2. Simulate candidates thetas &lt;- runif(10000, 0, 10) sims &lt;- sapply(thetas, function(theta) mean(rnorm(50, theta, 2))) df &lt;- data.frame(theta = thetas, s_sim = sims) # 3. Histogram of summary statistics ggplot(df, aes(x = s_sim)) + geom_histogram(binwidth = 0.1, fill = &quot;gray80&quot;, color = &quot;white&quot;) + geom_vline(xintercept = s_obs, color = &quot;red&quot;, size = 1) + labs( title = &quot;Simulated Means vs. Observed Mean&quot;, x = &quot;Simulated Mean&quot;, y = &quot;Count&quot; ) 10.7 Acceptance Threshold Visualization epsilon &lt;- 0.1 df$accepted &lt;- abs(df$s_sim - s_obs) &lt; epsilon ggplot(df, aes(x = theta, y = s_sim, color = accepted)) + geom_point(alpha = 0.5) + scale_color_manual(values = c(&quot;FALSE&quot; = &quot;gray70&quot;, &quot;TRUE&quot; = &quot;steelblue&quot;)) + geom_hline(yintercept = s_obs, linetype = &quot;dashed&quot;) + labs( title = &quot;ABC Acceptance Region&quot;, x = expression(theta), y = &quot;Simulated Mean&quot;, color = &quot;Accepted&quot; ) 10.8 ABC Approximate Posterior post &lt;- df$theta[df$accepted] ggplot(data.frame(theta = post), aes(x = theta)) + geom_density(fill = &quot;skyblue&quot;, alpha = 0.6) + labs( title = &quot;ABC Approximate Posterior for θ&quot;, x = expression(theta), y = &quot;Density&quot; ) 10.9 Extensions of ABC ABC‐SMC: Sequential Monte Carlo to adaptively reduce \\(\\varepsilon\\) ABC‐MCMC: MCMC scheme with ABC acceptance step Regression Adjustment: post‐processing to correct bias in posterior 10.10 References about ABC Beaumont, M. A., Zhang, W., &amp; Balding, D. J. (2002). Approximate Bayesian computation in population genetics. Genetics, 162(4), 2025–2035. Marin, J.-M., Pudlo, P., Robert, C. P., &amp; Ryder, R. J. (2012). Approximate Bayesian computational methods. Statistics and Computing, 22(6), 1167–1180. Sisson, S. A., Fan, Y., &amp; Beaumont, M. (2018). Handbook of Approximate Bayesian Computation. CRC Press. 10.11 Classification-Based LFI Simulate \\(\\{\\theta_i,\\,x_i\\}\\) from the prior and simulator Label data: \\(y=1\\) for \\(x_i\\sim p(x\\mid\\theta_i)\\) close to \\(x_{\\rm obs}\\) \\(y=0\\) for simulated from other \\(\\theta\\) Train a discriminator \\(D(x)\\approx P(y=1\\mid x)\\) Use \\(D(x_{\\rm obs})\\) (or its logit) as a surrogate likelihood 10.12 Algorithm Sketch Pseudo-code: for (t in 1:T) { θ_samples &lt;- sample_prior(N) x_sims &lt;- simulator(θ_samples) y_labels &lt;- ifelse(distance(x_sims, x_obs) &lt; ε_t, 1, 0) D_t &lt;- train_classifier(x_sims, y_labels) # Use D_t to weight or propose new θ in SMC/PMC scheme } 10.13 Motivation Many complex simulators → no tractable likelihood Approximate Bayesian Computation (ABC) sidesteps likelihood by simulating and comparing summaries Manual choice of summaries and thresholds can be ad hoc Gutmann et al. (2018) propose using a classifier to distinguish observed vs. simulated, turning that into a discrepancy 10.14 Likelihood-Free Inference: The Big Picture Simulator: black-box generative model \\[p(x \\mid \\theta)\\] Observed data: \\[x_{\\mathrm{obs}}\\] Goal: Approximate posterior \\[p(\\theta \\mid x_{\\mathrm{obs}})\\] without evaluating \\[p(x \\mid \\theta)\\] 10.15 Classification-Based LFI Draw \\((\\theta_i, x_i)_{i=1}^N\\) from the prior and simulator Label each \\(x_i\\): \\(y_i = 1\\) if \\(x_i\\) is “close” to \\(x_{\\mathrm{obs}}\\) \\(y_i = 0\\) otherwise Train classifier \\(D(x)\\approx P(y=1 \\mid x)\\) Use classifier output \\(D(x_{\\mathrm{obs}})\\) or its logit as a surrogate likelihood 10.16 Algorithm Sketch Pseudocode for (t in seq_len(T)) { theta &lt;- sample_prior(N) x_sim &lt;- simulator(theta) y &lt;- as.integer(distance(x_sim, x_obs) &lt; eps[t]) D &lt;- train_classifier(x_sim, y) # Use D to weight or propose new theta in an SMC/PMC loop } 10.17 Example: 1D Gaussian Mixture set.seed(123) n &lt;- 500; p &lt;- 0.3 # Observed: two-component Gaussian mixture obs &lt;- data.frame( x = c(rnorm(n*p, -2, 1), rnorm(n*(1-p), 3, 0.5)), label = &quot;Observed&quot; ) # Simulated under wrong single-Gaussian model sim &lt;- data.frame( x = rnorm(n, 0.5, 1.5), label = &quot;Simulated&quot; ) df &lt;- rbind(obs, sim) # Scatter plot library(ggplot2) ggplot(df, aes(x = x, y = 0, color = label)) + geom_jitter(height = 0.1, alpha = 0.6) + theme_minimal() + labs(title = &quot;Observed vs. Simulated Samples&quot;, x = &quot;Value&quot;, y = &quot;&quot;, color = &quot;&quot;) 10.17.1 Example: Logistic Discriminator \\[ D(x) = \\mathrm{logit}^{-1}\\bigl(\\beta_0 + \\beta_1 x\\bigr) \\] # Fit logistic regression df$y_bin &lt;- ifelse(df$label == &quot;Observed&quot;, 1, 0) fit &lt;- glm(y_bin ~ x, data = df, family = binomial) # ROC curve library(pROC) probs &lt;- predict(fit, type = &quot;response&quot;) roc_obj &lt;- roc(df$y_bin, probs) auc_val &lt;- auc(roc_obj) # Plot ROC roc_df &lt;- data.frame( fpr = rev(roc_obj$specificities), tpr = rev(roc_obj$sensitivities) ) library(ggplot2) ggplot(roc_df, aes(x = fpr, y = tpr)) + geom_line(linewidth = 1) + geom_abline(lty = 2) + theme_minimal() + labs(title = sprintf(&quot;ROC Curve (AUC = %.2f)&quot;, auc_val), x = &quot;False Positive Rate&quot;, y = &quot;True Positive Rate&quot;) 10.18 Applications &amp; Case Studies Population genetics: coalescent simulators for demographic inference Systems biology: stochastic models of biochemical networks Epidemiology: agent-based epidemic simulators 10.19 Practical Tips Classifier choice: Logistic regression for speed Random forests / GBMs for nonlinearity Neural nets for high-dimensional data Features: raw data vs. summary statistics Simulation budget: trade-off between classifier accuracy and compute cos Sequential schemes: embed into SMC-ABC to adapt \\(\\varepsilon\\) 10.20 Advantages &amp; Limitations Pros Cons Automatic, data-driven discrepancy Classifier may fit simulator quirks Continuous distance (AUC, logit) Extra training overhead Integrates into ABC/SMC frameworks Requires careful label calibration 10.21 Summary &amp; Outlook Classification reframes ABC as a supervised learning problem Provides continuous, interpretable discrepancy measures Future directions: Deep architectures for high-dimensional simulators Amortized inference for rapid repeated queries Hybrid likelihood-based &amp; classification approaches 10.22 References Gutmann, M. U., Dutta, R., Kaski, S., &amp; Corander, J. (2018). Likelihood-Free Inference via Classification. Statistics and Computing. Cranmer, K., Brehmer, J., &amp; Louppe, G. (2020). The Frontier of Simulation-Based Inference. PNAS. Papamakarios, G. &amp; Murray, I. (2016). Fast ε-Free Inference of Simulation Models with Bayesian Conditional Density Estimation. NeurIPS. Gutmann, M. U. &amp; Hyvärinen, A. (2012). Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models. AISTATS. "],["all-the-tools-already-available.html", "11 All the tools already available 11.1 A CRAN view of what exists: 11.2 Specific Packages:", " 11 All the tools already available 11.1 A CRAN view of what exists: https://cran.r-project.org/web/views/ExperimentalDesign.html 11.2 Specific Packages: acebayes: Optimal Bayesian Experimental Design using the ACE Algorithm adaptDiag: Bayesian Adaptive Designs for Diagnostic Trials ADCT: Adaptive Design in Clinical Trials adoptr: Adaptive Optimal Two-Stage Designs in R adpss: Design and Analysis of Locally or Globally Efficient Adaptive Designs agricolae: Statistical Procedures for Agricultural Research agricolaeplotr: Visualization of Design of Experiments from the ‘agricolae’ Package AlgDesign: Algorithmic Experimental Design ALTopt: Optimal Experimental Designs for Accelerated Life Testing Aoptbdtvc: A-Optimal Block Designs for Comparing Test Treatments with Controls asd: Simulations for Adaptive Seamless Designs bacistool: Bayesian Classification and Information Sharing (BaCIS) Tool for the Design of Multi-Group Phase II Clinical Trials BayesCR: Bayesian Analysis of Censored Regression Models Under Scale Mixture of Skew Normal Distributions bayesCT: Simulation and Analysis of Adaptive Bayesian Clinical Trials BayesDesign: Bayesian Single-Arm Design with Survival Endpoints BayesPPD: Bayesian Power Prior Design BDEsize: Efficient Determination of Sample Size in Balanced Design of Experiments bdlp: Transparent and Reproducible Artificial Data Generation bioOED: Sensitivity Analysis and Optimum Experiment Design for Microbial Inactivation deepgp: Sequential Design for Deep Gaussian Processes using MCMC DelayedEffect.Design: Sample Size and Power Calculations using the APPLE and SEPPLE Methods demu: Optimal Design Emulators via Point Processes DiceDesign: Designs of Computer Experiments DiceKriging: Kriging Methods for Computer Experiments DiceOptim: Kriging-Based Optimization for Computer Experiments earlyR: Estimation of Transmissibility in the Early Stages of a Disease Outbreak easypower: Sample Size Estimation for Experimental Designs experDesign: Design Experiments for Batches faux: Simulation for Factorial Designs FielDHub: A Shiny App for Design of Experiments in Life Sciences FieldSim: Random Fields (and Bridges) Simulations geospt: Geostatistical Analysis and Design of Optimal Spatial Sampling Networks gscounts: Group Sequential Designs with Negative Binomial Outcomes hiPOD: hierarchical Pooled Optimal Design MCPModPack: Simulation-Based Design and Analysis of Dose-Finding Trials scDesign: Simulation-based scRNA-seq experimental design phyclust: (seqgen) Simulate the evolution of nucleotide or amino acid sequences along a phylogeny splatter: Simulation of single-cell RNA sequencing count data MSstatsSampleSize: Simulation tool for optimal design of high-dimensional MS-based proteomics experiment PROPER: Simulation based methods for evaluating the statistical power in differential expression analysis from RNA-seq data MOSim: Simulates multi-omic experiments that mimic regulatory mechanisms within the cell mbtransfer: Simulation of hypothetical community trajectories under user-specified perturbations. miniLNM :For fitting and using logistic-normal multinomial models. It wraps a simple ‘Stan’ script. multimedia : Multimodal mediation analysis of microbiome data scDesign2 "],["multimodal-data-examples-of-approaches.html", "12 Multimodal data examples of approaches 12.1 Special data structures", " 12 Multimodal data examples of approaches There are some good examples for the microbiome that care instructuve for other domains. Multimedia website install.packages(&quot;multimedia&quot;) ####or you can use the new version: devtools::install_github(&quot;krisrs1128/multimedia&quot;) BiocManager::install(&quot;phyloseq&quot;) 12.1 Special data structures Summarized Experiment About MultiAssayExperiments About phyloseq Lecture on Microbiome Data Computational Challenges of Multidomain Data Complex dependent Factors for Multiway Data "],["other-examples-of-applications-to-the-microbiome.html", "13 Other examples of applications to the microbiome", " 13 Other examples of applications to the microbiome Kris Sankaran’s workshop on simulation for microbiome Kris Sankaran RPUBS site on simulation for biology Kris Sankaran: Semi synthetic data enrichment People ask me about why compositional methods don’t apply to the microbiome, there is a simple answer that the data are not compositional and there are too many boxes for the transformed dat to fit into that model. But there are other reasons not to use them, see: Why we shouldn’t be using compositional methods for Microbiome or Transcriptome data "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
